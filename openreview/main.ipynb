{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42f400c1",
   "metadata": {},
   "source": [
    "# Assignment 3: Extracting ‚ÄúBetter‚ÄêPaper‚Äù Guidelines from OpenReview dataset\n",
    "\n",
    "This notebook documents the steps, decisions, and code used to leverage local LLMs for mining common reviewer feedback and building a markdown of best practices.\n",
    "In fact, since I don‚Äôt have deep expertise in ML, I leaned heavily on LLMs to do the heavy work of extracting tips from peer reviews. I used ollama first with DeepSeek-8B, but because of its poor answers, I switched to Qwen‚Äë3-14B.\n",
    "Here's an example of Qwen‚Äë3-14B extracting some common concerns from the first three reviews of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f199ccfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, I need to help the user create markdown guidelines for authors based on the reviews they provided. Let me start by analyzing the three reviews to identify common issues and areas for improvement.\n",
      "\n",
      "First, Review 1 points out that the paper lacks rigorous experimentation. They didn't compare with earlier works like Joseph et al. and Wang et al., and didn't test on other domains like text classification. Also, there's a lack of theoretical analysis and some results are not conclusive. The abstract's claim about a 20.5% reduction in test error isn't addressed properly. They also mention issues with ablation studies and the need for more analysis on the warm-up phase and the selection pressure parameter.\n",
      "\n",
      "Review 2 mentions that the experiments don't show state-of-the-art results on standard datasets like MNIST, CIFAR-10, and CIFAR-100. The author is concerned about the method's effectiveness and suggests testing on larger datasets like Tiny ImageNet. There are also questions about the algorithm's convergence and theoretical guarantees. The review also points out that the entropy estimation becomes harder with more classes and asks if there are ways to mitigate that without increasing the window size. They also want more explanation on equation (4).\n",
      "\n",
      "Review 3 highlights that the method is heuristic and not well-justified theoretically. The experiments are limited to three datasets, which makes the results less conclusive. The introduction of new hyperparameters (like the annealing schedule) is noted, and the paper doesn't provide enough analysis on their impact. The reviewer would have given a stronger acceptance if there were more empirical or theoretical evidence.\n",
      "\n",
      "So, common mistakes from these reviews include:\n",
      "- Lack of thorough comparison with prior work.\n",
      "- Insufficient experimental rigor (e.g., limited datasets, no ablation studies).\n",
      "- Missing theoretical analysis.\n",
      "- Inadequate results discussion (e.g., not addressing claims, unclear results).\n",
      "- Poor explanation of key equations and hyperparameters.\n",
      "- Limited generalizability (only tested on image classification, not other domains).\n",
      "\n",
      "Now, the user wants a markdown guideline that addresses these points. The structure should have sections like Rigorous Experimentation, Comparison to Prior Work, Theoretical Justification, etc. Each section should list the common mistakes and best practices.\n",
      "\n",
      "I need to make sure each section is clear, uses the examples from the reviews, and provides actionable solutions. For instance, under Rigorous Experimentation, include comparing with relevant prior work, testing on diverse datasets, and conducting ablation studies. Under Theoretical Justification, mention the need for analysis of the method's properties and convergence guarantees.\n",
      "\n",
      "Also, the user mentioned including common mistakes and best solutions. Each section should have a heading, a brief description of the problem, and then bullet points with solutions. The markdown should be structured with headings, subheadings, and lists.\n",
      "\n",
      "I should check if the example provided by the user is followed. The example starts with # Paper Writing Guidelines, then sections with emojis and headings. Each section has a problem description and best practices as bullet points. I need to mirror that structure.\n",
      "\n",
      "Let me structure each section based on the identified issues. For example:\n",
      "\n",
      "## üß™ Rigorous Experimentation\n",
      "- Common mistakes: Not comparing with prior work, limited datasets, no ablation studies.\n",
      "- Solutions: Compare with relevant works, use diverse datasets, perform ablation studies.\n",
      "\n",
      "Similarly for other sections like Theoretical Justification, Results Interpretation, etc.\n",
      "\n",
      "I need to ensure that each part is concise and directly addresses the points from the reviews. Also, make sure that the markdown uses the correct syntax, like # for main headings, ## for subheadings, and bullet points with * or -.\n",
      "\n",
      "Finally, review the markdown to ensure it's comprehensive, covers all key areas from the reviews, and is easy to follow for authors looking to improve their paper writing.\n",
      "</think>\n",
      "\n",
      "# üìù Paper Writing Guidelines for Authors\n",
      "\n",
      "This guide outlines key principles to improve the quality and impact of your research papers, based on common issues identified in peer reviews.\n",
      "\n",
      "## üß™ Rigorous Experimentation  \n",
      "Many submissions fall short on experimental rigor‚Äîinsufficient controls, lack of repeatability, vague protocol descriptions, or no statistical power analysis.  \n",
      "\n",
      "**Best-Practice Solutions:**  \n",
      "1. **Compare with All Relevant Prior Work**  \n",
      "   * Include benchmarks from both recent and foundational works (e.g., Joseph et al., 2019; Wang et al., 2019).  \n",
      "   * Avoid cherry-picking competitors; evaluate against methods with similar goals.  \n",
      "2. **Test Across Diverse Domains and Tasks**  \n",
      "   * Validate methods on non-image tasks (e.g., text classification) to demonstrate generalizability.  \n",
      "   * Use large-scale datasets (e.g., Tiny ImageNet) to stress-test performance.  \n",
      "3. **Conduct Ablation Studies**  \n",
      "   * Analyze the impact of hyperparameters (e.g., selection pressure, warm-up phase).  \n",
      "   * Include baselines with and without key components (e.g., sliding window vs. growing window).  \n",
      "4. **Address Limitations in Results**  \n",
      "   * Clearly explain discrepancies (e.g., why online batch outperforms your method).  \n",
      "   * Justify claims (e.g., the 20.5% test error reduction) with detailed results.  \n",
      "\n",
      "## üìö Theoretical Justification  \n",
      "Weak theoretical analysis can undermine the novelty and impact of your method.  \n",
      "\n",
      "**Best-Practice Solutions:**  \n",
      "1. **Link to Existing Theories**  \n",
      "   * Connect your method to established frameworks (e.g., active learning, uncertainty quantification).  \n",
      "   * Analyze properties like expected model variance or convergence guarantees.  \n",
      "2. **Provide Intuition with Simpler Models**  \n",
      "   * Demonstrate the method‚Äôs validity on simple models (e.g., logistic regression).  \n",
      "   * Use visualizations (e.g., decision boundaries) to clarify intuition.  \n",
      "3. **Address Algorithmic Guarantees**  \n",
      "   * Prove convergence under certain conditions (e.g., stability of uncertainty estimates).  \n",
      "   * Discuss how hyperparameters (e.g., annealing schedules) affect behavior.  \n",
      "\n",
      "## üìä Results Interpretation  \n",
      "Vague or inconclusive results can weaken the paper‚Äôs contribution.  \n",
      "\n",
      "**Best-Practice Solutions:**  \n",
      "1. **Quantify and Contextualize Improvements**  \n",
      "   * Highlight statistical significance and task-specific benefits (e.g., \"20.5% reduction in test error on CIFAR-10\").  \n",
      "   * Compare results to baselines in both relative and absolute terms.  \n",
      "2. **Analyze Edge Cases and Anomalies**  \n",
      "   * Investigate unexpected patterns (e.g., sharp dips in loss curves).  \n",
      "   * Explain how results vary with task difficulty or dataset size.  \n",
      "3. **Use Clear and Consistent Metrics**  \n",
      "   * Avoid ambiguous labels (e.g., \"image classification\" vs. \"image classification from scratch\").  \n",
      "   * Define all acronyms and notation (e.g., $ P(y_i/x_i; q) $).  \n",
      "\n",
      "## üß† Method Design and Explanation  \n",
      "Unclear or under-motivated methods can confuse readers and reviewers.  \n",
      "\n",
      "**Best-Practice Solutions:**  \n",
      "1. **Clarify Key Design Choices**  \n",
      "   * Justify heuristic decisions (e.g., why a sliding window over a growing window?).  \n",
      "   * Explain hyperparameters (e.g., annealing schedules, window sizes).  \n",
      "2. **Simplify Complex Equations**  \n",
      "   * Break down formulas (e.g., equation (4)) with intuitive explanations.  \n",
      "   * Relate terms to known concepts (e.g., entropy, maximum likelihood).  \n",
      "3. **Highlight Novelty and Limitations**  \n",
      "   * Clearly state the method‚Äôs unique contributions (e.g., \"sliding window for recency bias\").  \n",
      "   * Acknowledge limitations (e.g., dependence on hyperparameters).  \n",
      "\n",
      "## üìÑ Writing and Presentation  \n",
      "Poorly written papers can obscure even strong results.  \n",
      "\n",
      "**Best-Practice Solutions:**  \n",
      "1. **Improve Clarity and Flow**  \n",
      "   * Use concise language and avoid jargon.  \n",
      "   * Structure sections to build a logical narrative (e.g., problem ‚Üí method ‚Üí results).  \n",
      "2. **Correct Technical Errors**  \n",
      "   * Proofread for typos (e.g., \"netowrks\" ‚Üí \"networks\").  \n",
      "   * Ensure consistency in notation and terminology.  \n",
      "3. **Visualize Key Insights**  \n",
      "   * Use graphs (e.g., loss convergence, entropy estimates) to support claims.  \n",
      "   * Label figures clearly and discuss their implications.  \n",
      "\n",
      "---  \n",
      "*Adhering to these guidelines can help you craft a more impactful, rigorous, and reader-friendly paper.*\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "question = r\"\"\"\n",
    "I am analizing a dataset of OpenReview papers because I want to create a small markdown of general guidelines, to help authors writing better papers.\n",
    "Here are three reviews of a paper I chose:\n",
    "<|review1|>\n",
    " This paper proposes Recency Bias, an adaptive mini batch selection method for training deep neural networks. To select informative minibatches for training, the proposed method maintains a fixed size sliding window of past model predictions for each data sample. At a given iteration, samples which have highly inconsistent predictions within the sliding window are added to the minibatch. The main contribution of this paper is the introduction of sliding window to remember past model predictions, as an improvement over the SOTA approach: Active Bias, which maintains a growing window of model predictions. Empirical studies are performed to show the superiority of Recency Bias over two SOTA approaches. Results are shown on the task of (1) image classification from scratch and (2) image classification by fine-tuning pretrained networks. +ves: + The idea of using a sliding window over a growing window in active batch selection is interesting. + Overall, the paper is well written. In particular, the Related Work section has a nice flow and puts the proposed method into context. Despite the method having limited novelty (sliding window instead of a growing window), the method has been well motivated by pointing out the limitations in SOTA methods. + The results section is well structured. It*s nice to see hyperparameter tuning results; and loss convergence graphs in various learning settings for each dataset. Concerns: - The key concern about the paper is the lack of rigorous experimentation to study the usefulness of the proposed method. Despite the paper stating that there have been earlier work (Joseph et al, 2019 and Wang et al, 2019) that attempt mini-batch selection, the paper does not compare with them. This is limiting. Further, since the proposed method is not specific to the domain of images, evaluating it on tasks other than image classification, such as text classification for instance, would have helped validate its applicability across domains. - Considering the limited results, a deeper analysis of the proposed method would have been nice. The idea of a sliding window over a growing window is a generic one, and there have been many efforts to theoretically analyze active learning over the last two decades. How does the proposed method fit in there? (For e.g., how does the expected model variance change in this setting?) Some form of theoretical/analytical reasoning behind the effectiveness of recency bias (which is missing) would provide greater insights to the community and facilitate further research in this direction. - The claim of 20.5% reduction in test error mentioned in the abstract has not been clearly addressed and pointed out in the results section of the paper. - On the same note, the results are not conclusively in favor of the proposed method, and only is marginally better than the competitors. Why does online batch perform consistently than the proposed method? There is no discussion of these inferences from the results. - The results would have been more complete if results were shown in a setting where just recency bias is used without the use of the selection pressure parameter. In other words, an ablation study on the effect of the selection pressure parameter would have been very useful. - How important is the warm-up phase to the proposed method? Considering the paper states that this is required to get good estimates of the quantization index of the samples, some ablation studies on reducing/increasing the warm-up phase and showing the results would have been useful to understand this. - Fig 4: Why are there sharp dips periodically in all the graphs? What do these correspond to? - The intuition behind the method is described well, however, the proposed method would have been really solidified if it were analysed in the context of a simple machine learning problem (such as logistic regression). As an example, verifying if the chosen minibatch samples are actually close to the decision boundary of a model (even if the model is very simple) would have helped analyze the proposed method well. Minor comments: * It would have been nice to see the relation between the effect of using recency bias and the difficulty of the task/dataset. * In the 2nd line in Introduction, it should be *deep networks* instead of *deep networks netowrks*. * Since both tasks in the experiments are about image classification, it would be a little misleading to present them as *image classification* and *finetuning*. A more informative way of titling them would be *image classification from scratch* and *image classification by finetuning*. * In Section 3.1, in the LHS of equation 3, it would be appropriate to use P(y_i/x_i; q) instead of P(y/x_i; q) since the former term was used in the paragraph. =====POST-REBUTTAL COMMENTS======== I thank the authors for the response and the efforts in the updated draft. Some of my queries were clarified. However, unfortunately, I still think more needs to be done to explain the consistency of the results and to study the generalizability of this work across datasets. I retain my original decision for these reasons.\n",
    "<|review1|>\n",
    "<|review2|>\n",
    "This paper proposes an interesting heuristic of batch construction from samples. Instead of the usual random sampling, the authors to sample based on some measures of the ``uncertainty‚Äù. To be specific, the uncertainty is measured as a normalized entropy estimated from a window of historical predictions. I like the idea of designing more sophisticated ways to encourage more exploration over the samples that the model is not good at. The thought is similar as active learning. It is interesting to see how similar thought can be used to improve the performance of the algorithm in the general batch gradient descent setting. On the other hand I am not quite convinced the proposed way is truly better. The main concern is the experiments do not quite show the state-of-the-art result at all. It is not even close on MNIST, CIFAR-10 and CIFAR-100. Also those datasets are relatively small one. Can authors add results on larger datasets such as tiny image net? Besides this main concern I also have some worries about the design of the algorithm. I listed them below: 1. The vanilla stochastic gradient descent can be roughly justified since the expectation of the stochastic gradient is the true gradient of the loss. Now with the proposed heuristic will this still be true? 2. Is there any guarantee the algorithm can converge? It is not clear to me as the optimization proceeds the ``uncertainty‚Äù may oscillate. Is there any condition when the convergence is guaranteed? 3. As the number of classes grows the estimation of the entropy itself is a tough problem. Is there any way to mitigate this issue other than increase the window size? Another minor comment: Could the authors add more explanation on equation (4)? For example, is related to the maximum entropy led by a uniform distribution, and the summation term in (4) is related to the empirical entropy.\n",
    "<|review2|>\n",
    "<|review3|>\n",
    "This paper explores a well motivated but very heuristic idea for selecting the next samples to train on for training deep learning models. This method relies on looking at the uncertainty of predictions of in the recent history of statements and preferring those instances that have a predictive uncertainty over the recent predictions. This allows the training method to train on instances that are neither too hard nor too easy and focus on reducing the uncertainty whenever it has the greatest potential gain to do so. There are two extra components that make this method work: - Windowing: only looking at the recent history of the instances which has two effects: firstly, the current state of the model is explored which gives a more recent assessment relative to the current state of the model. Secondly, it makes the algorithm faster by reducing the overhead of analyzing the prediction history of samples. - Annealing the selection bias: as the training goes on the selection becomes more random and less biased. This approach is evaluated in on three simple data-sets: MNIST, CIFAR-10 and CIFAR-100. Although this is a very limited subset of models, the results are consistent and statistically significant, although their effect is not really huge. The paper gives very little theoretical justification or analysis of the results but gives only the presented empirical evidence which seems to support the hypothesis on the efficacy of the approach. Another drawback of the approach is that it introduces new hyperparameters: those governing the annealing schedule for the selection bias. Since the approach seems efficient in a relatively constrained setup, it can be reasonably expected that it might be helpful in more general situations, therefore. On the other hand, since it is only evaluated on three very similar tasks, it limits the conclusiveness of the results. That*s why I would for weak accept. In the presence of more empirical (or even theoretical) evidence, I would vote for strong accept.\n",
    "<|review3|>\n",
    "Can you help me creating the markdown guidelines, inserting common mistakes that are worth citing and the best solution to fix them?\n",
    "I want something like:\n",
    "<|result|>\n",
    "# üìù Paper Writing Guidelines for Authors\n",
    "\n",
    "This guide outlines key principles to ...\n",
    "\n",
    "## üß™ Rigorous Experimentation\n",
    "\n",
    "Many submissions fall short on experimental rigor‚Äîinsufficient controls, lack of repeatability, vague protocol descriptions, or no statistical power analysis.\n",
    "\n",
    "**Best-Practice Solutions:**\n",
    "\n",
    "1. **Define Hypotheses & Variables**\n",
    "    * State null and alternative hypotheses explicitly.\n",
    "\n",
    "...\n",
    "\n",
    "## 2. üìä Comparison to Prior Work\n",
    "<|result|>\n",
    "\n",
    "Answer me with only the final guideline markdown.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "response = ollama.chat(model='qwen3:14b', messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": question\n",
    "    }\n",
    "])\n",
    "\n",
    "response = response['message']['content']\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c559f803",
   "metadata": {},
   "source": [
    "## üí° The initial idea\n",
    "At this point, the initial idea was to go through the reviews in batches of three (since each paper has three reviews), extract the main criticisms, and add them to a text file (already filled with some tips to guide the LLM) collecting all the feedback gathered so far. To do this, I would prompt the LLM to expand the markdown document if it identified any new suggestions. I thought also to maintain a list of best solutions that could be adopted to fix the concerns.\n",
    "\n",
    "Here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb3bb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user wants me to uniformize their markdown. Let me start by understanding the requirements.\n",
      "\n",
      "First, they mentioned that every heading should have an emoji. The original markdown already has emojis in the main headings like üß™, üìä, etc. But I need to check if all headings have emojis. The main title is \"# üìù Paper Writing Guidelines for Authors\" which has an emoji, and the subheadings like \"1. üß™ Rigorous Experimentation\" also have emojis. So that's covered.\n",
      "\n",
      "Next, they want no subheadings of the third level. Looking at the original markdown, the structure is main headings (##) and then numbered lists with subpoints. There are no third-level headings (###), so that's already in place. But I should confirm that the user doesn't want any, so maybe just ensure that the existing structure doesn't have any.\n",
      "\n",
      "They want to start with the title \"# üìù Paper Writing Guidelines for Authors\". The original markdown does that, so that's good.\n",
      "\n",
      "Then, they mentioned if there are headings that are too similar, they should be unified into a single heading. Looking at the sections, each heading is distinct: Rigorous Experimentation, Comparison to Prior Work, Quantitative Claims Need Proof, Dataset Size & Diversity. These are all different topics, so maybe no merging needed. But I should check if any sections can be combined. For example, \"Quantitative Claims Need Proof\" and \"Comparison to Prior Work\" both deal with presenting results, but they're separate enough. So probably no merging required.\n",
      "\n",
      "Now, the user wants a uniform style. The original uses emojis in main headings, but the subheadings under each section (like \"Best-Practice Solutions:\") don't have emojis. Should I add emojis to those? The user didn't specify, so maybe stick to the original structure unless instructed otherwise. The instruction says \"every heading have an emoji\", but the subheadings under the main sections are not headings, they're just bolded text. So maybe only the main headings (##) have emojis, which they already do.\n",
      "\n",
      "Also, the user wants to ensure no third-level headings. The original markdown uses ## for main sections and then lists, so that's fine.\n",
      "\n",
      "Now, looking at the markdown structure, the user might have had a lot of repetition in the sections, but I don't see that. Each section is about a different aspect: experimentation, comparison to prior work, quantitative claims, dataset size. So no merging needed.\n",
      "\n",
      "Another thing: the user mentioned \"if there are headings that are too similar uniform them in a single heading.\" So I need to check if any headings are similar. For example, \"Rigorous Experimentation\" and \"Dataset Size & Diversity\" are different. \"Comparison to Prior Work\" and \"Quantitative Claims Need Proof\" are different. So no merging needed.\n",
      "\n",
      "Now, the task is to apply the uniform style. The original markdown uses emojis in the main headings, bullet points with numbers, and code blocks. The user might want consistency in the use of emojis, maybe adding them to the subheadings, but the instruction says \"every heading have an emoji\", which in the original are the main headings. So I think the main headings are the only ones with emojis, and the rest are just lists.\n",
      "\n",
      "So the final step is to ensure that all main headings have emojis, no third-level headings, and that similar headings are merged if needed. Since there's no merging needed, the main task is to make sure the structure is consistent and follows the user's example.\n",
      "\n",
      "I'll proceed to format the markdown accordingly, ensuring that each main heading has an emoji, no third-level headings are present, and the structure is uniform.\n",
      "</think>\n",
      "\n",
      "# üìù Paper Writing Guidelines for Authors\n",
      "\n",
      "This guide outlines key principles to help authors avoid common pitfalls and meet the expectations of peer reviewers and the broader research community.\n",
      "\n",
      "## üß™ Rigorous Experimentation\n",
      "\n",
      "Many submissions fall short on experimental rigor‚Äîinsufficient controls, lack of repeatability, vague protocol descriptions, or no statistical power analysis.\n",
      "\n",
      "**Best-Practice Solutions:**\n",
      "\n",
      "1. **Define Hypotheses & Variables**\n",
      "\n",
      "   * State null and alternative hypotheses explicitly.\n",
      "   * Identify independent, dependent, and control variables.\n",
      "\n",
      "2. **Use Proper Controls & Randomization**\n",
      "\n",
      "   * When possible, include both positive and negative controls.\n",
      "   * Randomly assign subjects or data batches to conditions to avoid bias.\n",
      "\n",
      "3. **Replicate & Report Variability**\n",
      "\n",
      "   * Run ‚â• 3 independent trials; report means ¬± standard deviations.\n",
      "   * If variation exceeds ~5%, investigate sources before concluding.\n",
      "\n",
      "4. **Perform Power Analysis**\n",
      "\n",
      "   * Calculate required sample size up-front to detect your expected effect size (e.g., Cohen's d = 0.4).\n",
      "   * Document alpha (commonly 0.05) and desired power (commonly 0.8).\n",
      "\n",
      "5. **Pre-register & Share Protocols**\n",
      "\n",
      "   * When journal or funder policies allow, register your design on platforms like OSF.\n",
      "   * Include detailed methods (e.g., scripts, hardware specs) in supplements.\n",
      "\n",
      "## üìä Comparison to Prior Work\n",
      "\n",
      "Authors often claim novelty without fair, head-to-head benchmarks against the most relevant baselines.\n",
      "\n",
      "**Best-Practice Solutions:**\n",
      "\n",
      "1. **Conduct a Focused Literature Review**\n",
      "\n",
      "   * Identify 3-5 seminal and recent papers tackling the same problem.\n",
      "   * Highlight gaps your approach fills.\n",
      "\n",
      "2. **Re-implement or Use Published Code**\n",
      "\n",
      "   * Whenever possible, obtain or re-implement baseline methods rather than quoting old results.\n",
      "   * Match preprocessing, hyperparameters, and evaluation metrics exactly.\n",
      "\n",
      "3. **Present Results in Comparative Tables**\n",
      "\n",
      "   ```markdown\n",
      "   | Method        | Dataset     | Metric     | Result         |\n",
      "   |---------------|-------------|------------|----------------|\n",
      "   | Proposed      | X           | Accuracy   | 87.4 %         |\n",
      "   | Baseline A    | X           | Accuracy   | 82.1 %         |\n",
      "   | Baseline B    | X           | Accuracy   | 84.3 %         |\n",
      "   ```\n",
      "\n",
      "   * Include confidence intervals or standard errors for each entry.\n",
      "\n",
      "4. **Discuss Trade-Offs**\n",
      "\n",
      "   * If your method improves one metric but worsens another (e.g., speed vs. accuracy), analyze why.\n",
      "   * Be transparent about limitations relative to baselines.\n",
      "\n",
      "## üîç Quantitative Claims Need Proof\n",
      "\n",
      "‚Äú20% improvement‚Äù sounds impressive‚Äîuntil readers realize there's no reported variance, p-value, or context for that number.\n",
      "\n",
      "**Best-Practice Solutions:**\n",
      "\n",
      "1. **State Claims Early & Precisely**\n",
      "\n",
      "   * In the Abstract/Introduction, quantify your main gain (e.g., ‚ÄúOur model reduces error by 20%.‚Äù).\n",
      "\n",
      "2. **Prove Them in the Results Section**\n",
      "\n",
      "   * Show tabular or graphical results with error bars (95% CI) or p-values.\n",
      "   * Example:\n",
      "\n",
      "     ```markdown\n",
      "     | Model         | Mean Error | 95% CI      | p-value |\n",
      "     |---------------|------------|--------------|---------|\n",
      "     | Ours          | 0.12       | [0.10, 0.14] | 0.003   |\n",
      "     | Baseline      | 0.15       | [0.13, 0.17] | ‚Äî       |\n",
      "     ```\n",
      "\n",
      "3. **Use Appropriate Statistical Tests**\n",
      "\n",
      "   * T-tests, ANOVA, or non-parametric tests depending on distribution.\n",
      "   * Correct for multiple hypotheses when reporting several metrics.\n",
      "\n",
      "4. **Explain Practical Impact**\n",
      "\n",
      "   * Beyond percentages, describe what a 20% reduction means in domain terms (e.g., 2 days saved in processing time).\n",
      "\n",
      "## üìâ Dataset Size & Diversity\n",
      "\n",
      "Too-small or homogenous datasets lead to overfitting and findings that don't generalize.\n",
      "\n",
      "**Best-Practice Solutions:**\n",
      "\n",
      "1. **Follow ‚ÄúRule of Ten‚Äù When Feasible**\n",
      "\n",
      "   * Aim for ‚â• 10 samples per feature dimension (e.g., 50 features ‚Üí ‚â• 500 samples).\n",
      "\n",
      "2. **Leverage Power-Based Sample Size Calculations**\n",
      "\n",
      "   * For a medium effect (d = 0.4), two groups need ~100 samples each.\n",
      "\n",
      "3. **Document Dataset Splits & Diversity**\n",
      "\n",
      "   * Report train/validation/test sizes and demographic or source diversity (e.g., multiple sites, time periods).\n",
      "\n",
      "4. **Augment When Necessary**\n",
      "\n",
      "   * If real-world data are limited, consider data augmentation, transfer learning, or synthetic data‚Äîalways flag these in methods.\n",
      "\n",
      "5. **Assess Generalizability**\n",
      "\n",
      "   Perform sensitivity analyses: how do results change if you drop 10-20% of data or vary domain characteristics?\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import openpyxl\n",
    "from io import SEEK_END\n",
    "\n",
    "# this file contains the template result we want to get from the model\n",
    "starting_result = open(\"starting_result.txt\", \"r\")\n",
    "final_result = open(\"final_result.txt\", \"w+\")\n",
    "final_result.write(starting_result.read())\n",
    "final_result.seek(0, SEEK_END)\n",
    "starting_result.close()\n",
    "\n",
    "def get_main_question(result, review1, review2, review3):\n",
    "    return f\"\"\"I am analizing a dataset of OpenReview papers because I want to create a small markdown of general guidelines, to help authors writing better papers.\n",
    "So I collected some reviews and this is the result I created:\n",
    "<|result|>\n",
    "{result}\n",
    "<|result|>\n",
    "Now I have three reviews of a paper I chose.\n",
    "I want you to find any new common mistakes that are worth citing and add them as a new heading to the markdown I provided you. Mantain the original style of the markdown (with headings, emoji in the headings, no headings of third level, etc.). If the mistakes are already present, do not add them. If you don't find any new mistake, leave the markdown as before.\n",
    "Here are the reviews:\n",
    "<|review1|>\n",
    "{review1}\n",
    "<|review1|>\n",
    "<|review2|>\n",
    "{review2}\n",
    "<|review2|>\n",
    "<|review3|>\n",
    "{review3}\n",
    "<|review3|>\n",
    "Answer me with only the final guideline markdown.\n",
    "\"\"\"\n",
    "\n",
    "sheet = openpyxl.load_workbook(\"or_2020.xlsx\").active\n",
    "\n",
    "for row in range(5, 40, 3): # there should be sheet.max_row, but with my pc would take ~80 hours\n",
    "    review1 = sheet.cell(row, 6).value\n",
    "    review2 = sheet.cell(row+1, 6).value\n",
    "    review3 = sheet.cell(row+2, 6).value\n",
    "    final_question = get_main_question(final_result.read(), review1, review2, review3)\n",
    "    final_result.seek(0, SEEK_END)\n",
    "    response = ollama.chat(model='qwen3:14b', messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": final_question\n",
    "        }\n",
    "    ])\n",
    "    response = response['message']['content']\n",
    "    if response.find('```markdown') != -1:\n",
    "        response = response.split('```markdown')[1].split('```')[0] # get the guideline markdown part\n",
    "    elif response.find('#') != -1:\n",
    "        response = response.split('#')[1]\n",
    "    final_result.write(response)\n",
    "\n",
    "final_result.seek(0)\n",
    "uniform_question = f\"\"\"\n",
    "Uniform this markdown to have a uniform style (for example every heading have an emoji, no subheading of third level, start with the title \"# üìù Paper Writing Guidelines for Authors\", etc). Finally, if there are headings that are too similar uniform them in a single heading.\n",
    "Here's the markdown:\n",
    "<|markdown|>\n",
    "{final_result.read()}\n",
    "<|markdown|>\n",
    "\"\"\"\n",
    "response = ollama.chat(model='qwen3:14b', messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": uniform_question\n",
    "    }\n",
    "])\n",
    "response = response['message']['content']\n",
    "final_result.seek(0, SEEK_END)\n",
    "final_result.write(response)\n",
    "print(response)\n",
    "final_result.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0ade20",
   "metadata": {},
   "source": [
    "The result turned out to be not too bad, but some problems arised:\n",
    "\n",
    "- **Prompt‚Äêlength explosion:** as the markdown grows, the LLM allucinates or cuts off.\n",
    "- **Inconsistency:** later generations sometimes rephrase or lose earlier entries.\n",
    "- **Throughput:** re‚Äësending the entire file for each triple becomes slow.\n",
    "\n",
    "For this reason, I decided to revise the strategy.\n",
    "\n",
    "## üîÑ Revised Strategy: Separate Extraction & Clustering\n",
    "\n",
    "Rather than carrying forward the cumulative output and feeding it back to the LLM each time, I decided to extract the common concerns from each triplet of reviews and save them to a file. Once all reviews are processed, I generate a neatly formatted Markdown document that lists each \"cluster\" of concerns alongside recommended best practices for addressing them. I also discovered that Ollama lets you include a chat history, so that each time I ask a question I can attach an example of extracting common guidelines, providing a bit of context.\n",
    "\n",
    "Here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc768f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# üìù Paper Writing Guidelines for Authors\n",
      "\n",
      "This guide outlines key principles to help authors avoid common pitfalls and meet the expectations of peer reviewers and the broader research community.\n",
      "\n",
      "## 1. üß™ Rigorous Experimentation\n",
      "\n",
      "Many submissions fall short on experimental rigor‚Äîinsufficient controls, lack of repeatability, vague protocol descriptions, or no statistical power analysis.\n",
      "\n",
      "**Best-Practice Solutions:**\n",
      "\n",
      "1. **Define Hypotheses & Variables**\n",
      "\n",
      "   * State null and alternative hypotheses explicitly.\n",
      "   * Identify independent, dependent, and control variables.\n",
      "\n",
      "2. **Use Proper Controls & Randomization**\n",
      "\n",
      "   * When possible, include both positive and negative controls.\n",
      "   * Randomly assign subjects or data batches to conditions to avoid bias.\n",
      "\n",
      "3. **Replicate & Report Variability**\n",
      "\n",
      "   * Run ‚â• 3 independent trials; report means ¬± standard deviations.\n",
      "   * If variation exceeds \\~5%, investigate sources before concluding.\n",
      "\n",
      "4. **Perform Power Analysis**\n",
      "\n",
      "   * Calculate required sample size up-front to detect your expected effect size (e.g., Cohen's d = 0.4).\n",
      "   * Document alpha (commonly 0.05) and desired power (commonly 0.8).\n",
      "\n",
      "5. **Pre-register & Share Protocols**\n",
      "\n",
      "   * When journal or funder policies allow, register your design on platforms like OSF.\n",
      "   * Include detailed methods (e.g., scripts, hardware specs) in supplements.\n",
      "\n",
      "## 2. üìä Comparison to Prior Work\n",
      "\n",
      "Authors often claim novelty without fair, head-to-head benchmarks against the most relevant baselines.\n",
      "\n",
      "**Best-Practice Solutions:**\n",
      "\n",
      "1. **Conduct a Focused Literature Review**\n",
      "\n",
      "   * Identify 3-5 seminal and recent papers tackling the same problem.\n",
      "   * Highlight gaps your approach fills.\n",
      "\n",
      "2. **Re-implement or Use Published Code**\n",
      "\n",
      "   * Whenever possible, obtain or re-implement baseline methods rather than quoting old results.\n",
      "   * Match preprocessing, hyperparameters, and evaluation metrics exactly.\n",
      "\n",
      "3. **Present Results in Comparative Tables**\n",
      "\n",
      "   ```markdown\n",
      "   | Method        | Dataset     | Metric     | Result         |\n",
      "   |---------------|-------------|------------|----------------|\n",
      "   | Proposed      | X           | Accuracy   | 87.4 %         |\n",
      "   | Baseline A    | X           | Accuracy   | 82.1 %         |\n",
      "   | Baseline B    | X           | Accuracy   | 84.3 %         |\n",
      "   ```\n",
      "\n",
      "   * Include confidence intervals or standard errors for each entry.\n",
      "\n",
      "4. **Discuss Trade-Offs**\n",
      "\n",
      "   * If your method improves one metric but worsens another (e.g., speed vs. accuracy), analyze why.\n",
      "   * Be transparent about limitations relative to baselines.\n",
      "\n",
      "## 3. üîç Quantitative Claims Need Proof\n",
      "\n",
      "‚Äú20 % improvement‚Äù sounds impressive‚Äîuntil readers realize there's no reported variance, p-value, or context for that number.\n",
      "\n",
      "**Best-Practice Solutions:**\n",
      "\n",
      "1. **State Claims Early & Precisely**\n",
      "\n",
      "   * In the Abstract/Introduction, quantify your main gain (e.g., ‚ÄúOur model reduces error by 20 %.‚Äù).\n",
      "\n",
      "2. **Prove Them in the Results Section**\n",
      "\n",
      "   * Show tabular or graphical results with error bars (95 % CI) or p-values.\n",
      "   * Example:\n",
      "\n",
      "     ```markdown\n",
      "     | Model         | Mean Error | 95 % CI      | p-value |\n",
      "     |---------------|------------|--------------|---------|\n",
      "     | Ours          | 0.12       | [0.10, 0.14] | 0.003   |\n",
      "     | Baseline      | 0.15       | [0.13, 0.17] | ‚Äî       |\n",
      "     ```\n",
      "\n",
      "3. **Use Appropriate Statistical Tests**\n",
      "\n",
      "   * T-tests, ANOVA, or non-parametric tests depending on distribution.\n",
      "   * Correct for multiple hypotheses when reporting several metrics.\n",
      "\n",
      "4. **Explain Practical Impact**\n",
      "\n",
      "   * Beyond percentages, describe what a 20 % reduction means in domain terms (e.g., 2 days saved in processing time).\n",
      "\n",
      "## 4. üìâ Dataset Size & Diversity\n",
      "\n",
      "Too-small or homogenous datasets lead to overfitting and findings that don't generalize.\n",
      "\n",
      "**Best-Practice Solutions:**\n",
      "\n",
      "1. **Follow ‚ÄúRule of Ten‚Äù When Feasible**\n",
      "\n",
      "   * Aim for ‚â• 10 samples per feature dimension (e.g., 50 features ‚Üí ‚â• 500 samples).\n",
      "\n",
      "2. **Leverage Power-Based Sample Size Calculations**\n",
      "\n",
      "   * For a medium effect (d = 0.4), two groups need \\~100 samples each.\n",
      "\n",
      "3. **Document Dataset Splits & Diversity**\n",
      "\n",
      "   * Report train/validation/test sizes and demographic or source diversity (e.g., multiple sites, time periods).\n",
      "\n",
      "4. **Augment When Necessary**\n",
      "\n",
      "   * If real-world data are limited, consider data augmentation, transfer learning, or synthetic data‚Äîalways flag these in methods.\n",
      "\n",
      "5. **Assess Generalizability**\n",
      "\n",
      "   Perform sensitivity analyses: how do results change if you drop 10-20 % of data or vary domain characteristics?\n",
      "\n",
      "## 5. üß≠ Address Limitations of Key Method Components\n",
      "\n",
      "Proposed methods often rely on critical components (e.g., early stopping criteria, safe set construction rules) that may have inherent limitations. Failing to acknowledge these can undermine the robustness of the approach.\n",
      "\n",
      "**Best-Practice Solutions:**\n",
      "\n",
      "1. **Analyze and Discuss Limitations**\n",
      "\n",
      "   * Explicitly address potential weaknesses of your method‚Äôs core components (e.g., ‚ÄúEarly stopping based on validation error may fail if the validation set is small or unrepresentative‚Äù).\n",
      "\n",
      "2. **Propose Mitigations or Alternatives**\n",
      "\n",
      "   * Suggest ways to improve the component (e.g., ‚ÄúUsing a larger validation set or incorporating uncertainty estimates could mitigate overfitting to the validation set‚Äù).\n",
      "\n",
      "3. **Compare with Alternative Approaches**\n",
      "\n",
      "   * If your method‚Äôs component is unconventional, compare it with alternatives (e.g., ‚ÄúOur safe set construction differs from [1] by using [X] instead of [Y], which may improve robustness in [specific scenario]‚Äù).   \n",
      "\n",
      "## 6. üìö Justify Novelty Through Comprehensive Literature Review\n",
      "\n",
      "Even if a method is simple or similar to existing approaches, its contribution must be clearly justified by demonstrating gaps in prior work.\n",
      "\n",
      "**Best-Practice Solutions:**\n",
      "\n",
      "1. **Map Your Work to Existing Literature**\n",
      "\n",
      "   * Clearly state how your approach relates to prior work (e.g., ‚ÄúWhile [1] uses iterative training for label noise, our method introduces [X] to address [specific problem]‚Äù).\n",
      "\n",
      "2. **Highlight Novel Contributions**\n",
      "\n",
      "   * Emphasize what your method adds (e.g., ‚ÄúOur early stopping criterion is the first to incorporate [X], improving performance over [1] by [Y]%‚Äù).\n",
      "\n",
      "3. **Cite Missing Relevant Work**\n",
      "\n",
      "   * If prior work was overlooked, acknowledge it and explain how your method differs (e.g., ‚ÄúAlthough [1] proposed a similar safe set construction, our approach avoids [X] by [Y]‚Äù).\n",
      "\n",
      "## 7. üîç Validate Claims with Comprehensive and Reproducible Experiments\n",
      "\n",
      "Empirical claims must be supported by thorough, reproducible experiments that clearly demonstrate the method‚Äôs effectiveness and generalizability.\n",
      "\n",
      "**Best-Practice Solutions:**\n",
      "\n",
      "1. **Test on Diverse and Challenging Datasets**\n",
      "\n",
      "   * Include datasets that reflect real-world conditions (e.g., ‚ÄúClothing1M‚Äù for label noise studies) to demonstrate robustness.\n",
      "\n",
      "2. **Use Clear and Reproducible Evaluation Metrics**\n",
      "\n",
      "   * Ensure all metrics and baselines are well-defined and comparable (e.g., ‚ÄúUse clean validation sets for all methods to ensure fairness‚Äù).\n",
      "\n",
      "3. **Provide Detailed and Visual Comparisons**\n",
      "\n",
      "   * Use clear visualizations and summaries (e.g., scatterplots, tables with meaningful groupings) to support claims rather than overwhelming readers with raw data.\n",
      "\n",
      "## 8. üìå Clarify Definitions and Improve Readability\n",
      "\n",
      "Ambiguous terms, undefined acronyms, and unclear explanations can hinder understanding and reduce the impact of the work.\n",
      "\n",
      "**Best-Practice Solutions:**\n",
      "\n",
      "1. **Define All Terms and Acronyms**\n",
      "\n",
      "   * Clearly explain all technical terms and acronyms on first use (e.g., ‚ÄúSISR: Single Image Super-Resolution, HVS: Human Visual System‚Äù).\n",
      "\n",
      "2. **Structure Complex Ideas Clearly**\n",
      "\n",
      "   * Use well-organized explanations and avoid overly dense sections (e.g., ‚ÄúReorganize Sec. 4.3 to clarify the concept of visual masking‚Äù).\n",
      "\n",
      "3. **Use Visual and Tabular Aids to Improve Comprehension**\n",
      "\n",
      "   * Avoid large, unstructured tables and use visual aids or summaries to highlight key findings\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\~'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\~'\n",
      "/var/folders/gj/gmsvyb2j4mb68dqn3c3mhq6m0000gn/T/ipykernel_7872/2667602092.py:7: SyntaxWarning: invalid escape sequence '\\~'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import openpyxl\n",
    "from io import SEEK_END\n",
    "\n",
    "final_result = open(\"final_result.txt\", \"w+\")\n",
    "\n",
    "def get_main_question(review1, review2, review3):\n",
    "    return f\"\"\"Perfect, now I have other three reviews of a paper I chose:\n",
    "<|review1|>\n",
    "{review1}\n",
    "<|review1|>\n",
    "<|review2|>\n",
    "{review2}\n",
    "<|review2|>\n",
    "<|review3|>\n",
    "{review3}\n",
    "<|review3|>\n",
    "Answer me with only the common concern that the reviewers have about these three reviews, to help authors know in advance what are the common concerns. Answer with only the bullet points without any other explanation.\n",
    "\"\"\"\n",
    "\n",
    "question = r\"\"\"\n",
    "I am analizing a dataset of OpenReview papers with reviews of the papers because I want to extract some common concern that the reviewers have, to help authors know in advance what are the common concerns.\n",
    "In particular, I have three reviews of a paper I chose:\n",
    "<|review1|>\n",
    " This paper proposes Recency Bias, an adaptive mini batch selection method for training deep neural networks. To select informative minibatches for training, the proposed method maintains a fixed size sliding window of past model predictions for each data sample. At a given iteration, samples which have highly inconsistent predictions within the sliding window are added to the minibatch. The main contribution of this paper is the introduction of sliding window to remember past model predictions, as an improvement over the SOTA approach: Active Bias, which maintains a growing window of model predictions. Empirical studies are performed to show the superiority of Recency Bias over two SOTA approaches. Results are shown on the task of (1) image classification from scratch and (2) image classification by fine-tuning pretrained networks. +ves: + The idea of using a sliding window over a growing window in active batch selection is interesting. + Overall, the paper is well written. In particular, the Related Work section has a nice flow and puts the proposed method into context. Despite the method having limited novelty (sliding window instead of a growing window), the method has been well motivated by pointing out the limitations in SOTA methods. + The results section is well structured. It*s nice to see hyperparameter tuning results; and loss convergence graphs in various learning settings for each dataset. Concerns: - The key concern about the paper is the lack of rigorous experimentation to study the usefulness of the proposed method. Despite the paper stating that there have been earlier work (Joseph et al, 2019 and Wang et al, 2019) that attempt mini-batch selection, the paper does not compare with them. This is limiting. Further, since the proposed method is not specific to the domain of images, evaluating it on tasks other than image classification, such as text classification for instance, would have helped validate its applicability across domains. - Considering the limited results, a deeper analysis of the proposed method would have been nice. The idea of a sliding window over a growing window is a generic one, and there have been many efforts to theoretically analyze active learning over the last two decades. How does the proposed method fit in there? (For e.g., how does the expected model variance change in this setting?) Some form of theoretical/analytical reasoning behind the effectiveness of recency bias (which is missing) would provide greater insights to the community and facilitate further research in this direction. - The claim of 20.5% reduction in test error mentioned in the abstract has not been clearly addressed and pointed out in the results section of the paper. - On the same note, the results are not conclusively in favor of the proposed method, and only is marginally better than the competitors. Why does online batch perform consistently than the proposed method? There is no discussion of these inferences from the results. - The results would have been more complete if results were shown in a setting where just recency bias is used without the use of the selection pressure parameter. In other words, an ablation study on the effect of the selection pressure parameter would have been very useful. - How important is the warm-up phase to the proposed method? Considering the paper states that this is required to get good estimates of the quantization index of the samples, some ablation studies on reducing/increasing the warm-up phase and showing the results would have been useful to understand this. - Fig 4: Why are there sharp dips periodically in all the graphs? What do these correspond to? - The intuition behind the method is described well, however, the proposed method would have been really solidified if it were analysed in the context of a simple machine learning problem (such as logistic regression). As an example, verifying if the chosen minibatch samples are actually close to the decision boundary of a model (even if the model is very simple) would have helped analyze the proposed method well. Minor comments: * It would have been nice to see the relation between the effect of using recency bias and the difficulty of the task/dataset. * In the 2nd line in Introduction, it should be *deep networks* instead of *deep networks netowrks*. * Since both tasks in the experiments are about image classification, it would be a little misleading to present them as *image classification* and *finetuning*. A more informative way of titling them would be *image classification from scratch* and *image classification by finetuning*. * In Section 3.1, in the LHS of equation 3, it would be appropriate to use P(y_i/x_i; q) instead of P(y/x_i; q) since the former term was used in the paragraph. =====POST-REBUTTAL COMMENTS======== I thank the authors for the response and the efforts in the updated draft. Some of my queries were clarified. However, unfortunately, I still think more needs to be done to explain the consistency of the results and to study the generalizability of this work across datasets. I retain my original decision for these reasons.\n",
    "<|review1|>\n",
    "<|review2|>\n",
    "This paper proposes an interesting heuristic of batch construction from samples. Instead of the usual random sampling, the authors to sample based on some measures of the ``uncertainty‚Äù. To be specific, the uncertainty is measured as a normalized entropy estimated from a window of historical predictions. I like the idea of designing more sophisticated ways to encourage more exploration over the samples that the model is not good at. The thought is similar as active learning. It is interesting to see how similar thought can be used to improve the performance of the algorithm in the general batch gradient descent setting. On the other hand I am not quite convinced the proposed way is truly better. The main concern is the experiments do not quite show the state-of-the-art result at all. It is not even close on MNIST, CIFAR-10 and CIFAR-100. Also those datasets are relatively small one. Can authors add results on larger datasets such as tiny image net? Besides this main concern I also have some worries about the design of the algorithm. I listed them below: 1. The vanilla stochastic gradient descent can be roughly justified since the expectation of the stochastic gradient is the true gradient of the loss. Now with the proposed heuristic will this still be true? 2. Is there any guarantee the algorithm can converge? It is not clear to me as the optimization proceeds the ``uncertainty‚Äù may oscillate. Is there any condition when the convergence is guaranteed? 3. As the number of classes grows the estimation of the entropy itself is a tough problem. Is there any way to mitigate this issue other than increase the window size? Another minor comment: Could the authors add more explanation on equation (4)? For example, is related to the maximum entropy led by a uniform distribution, and the summation term in (4) is related to the empirical entropy.\n",
    "<|review2|>\n",
    "<|review3|>\n",
    "This paper explores a well motivated but very heuristic idea for selecting the next samples to train on for training deep learning models. This method relies on looking at the uncertainty of predictions of in the recent history of statements and preferring those instances that have a predictive uncertainty over the recent predictions. This allows the training method to train on instances that are neither too hard nor too easy and focus on reducing the uncertainty whenever it has the greatest potential gain to do so. There are two extra components that make this method work: - Windowing: only looking at the recent history of the instances which has two effects: firstly, the current state of the model is explored which gives a more recent assessment relative to the current state of the model. Secondly, it makes the algorithm faster by reducing the overhead of analyzing the prediction history of samples. - Annealing the selection bias: as the training goes on the selection becomes more random and less biased. This approach is evaluated in on three simple data-sets: MNIST, CIFAR-10 and CIFAR-100. Although this is a very limited subset of models, the results are consistent and statistically significant, although their effect is not really huge. The paper gives very little theoretical justification or analysis of the results but gives only the presented empirical evidence which seems to support the hypothesis on the efficacy of the approach. Another drawback of the approach is that it introduces new hyperparameters: those governing the annealing schedule for the selection bias. Since the approach seems efficient in a relatively constrained setup, it can be reasonably expected that it might be helpful in more general situations, therefore. On the other hand, since it is only evaluated on three very similar tasks, it limits the conclusiveness of the results. That*s why I would for weak accept. In the presence of more empirical (or even theoretical) evidence, I would vote for strong accept.\n",
    "<|review3|>\n",
    "Answer me with only the common concern that the reviewers have, with only the bullet points without any other explanation.\n",
    "\"\"\"\n",
    "\n",
    "answer = r\"\"\"\n",
    "- Rigorous Experimentation: Many submissions fall short on experimental rigor‚Äîinsufficient controls, lack of repeatability, vague protocol descriptions, or no statistical power analysis.\n",
    "- Comparison to Prior Work: Authors often claim novelty without fair, head-to-head benchmarks against the most relevant baselines.\n",
    "- Quantitative Claims Need Proof: Percentages are meaningless without a clear definition of the metric, baseline, and statistical significance. For example, \"Our method is 20% better\" is vague without context.\n",
    "- Dataset Size & Diversity: Too-small or homogenous datasets lead to overfitting and findings that don't generalize.\n",
    "\"\"\"\n",
    "\n",
    "sheet = openpyxl.load_workbook(\"or_2020.xlsx\").active\n",
    "\n",
    "for row in range(5, 40, 3): # there should be sheet.max_row, but with my pc would take ~80 hours\n",
    "    review1 = sheet.cell(row, 6).value\n",
    "    review2 = sheet.cell(row+1, 6).value\n",
    "    review3 = sheet.cell(row+2, 6).value\n",
    "    final_question = get_main_question(review1, review2, review3)\n",
    "    response = ollama.chat(model='qwen3:14b', messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": answer\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": final_question\n",
    "        }\n",
    "    ])\n",
    "    response = response['message']['content']\n",
    "    if response.find('</think>') != -1:\n",
    "        response = response.split('</think>')[1] # remove the think part\n",
    "    final_result.write(response)\n",
    "    final_result.seek(0, SEEK_END)\n",
    "\n",
    "final_result.seek(0)\n",
    "question = f\"\"\"\n",
    "I have analized a dataset of OpenReview papers with reviews of the papers because I wanted to extract some common concern that the reviewers have, to help authors know in advance what are the common concerns. Now I have the big list of general concerns, can you help me creating a nice markdown with the concerns and best solutions? For example, this can be a possible result:\n",
    "<|result|>\n",
    "# üìù Paper Writing Guidelines for Authors\n",
    "\n",
    "This guide outlines key principles to help authors avoid common pitfalls and meet the expectations of peer reviewers and the broader research community.\n",
    "\n",
    "## 1. üß™ Rigorous Experimentation\n",
    "\n",
    "Many submissions fall short on experimental rigor‚Äîinsufficient controls, lack of repeatability, vague protocol descriptions, or no statistical power analysis.\n",
    "\n",
    "**Best-Practice Solutions:**\n",
    "\n",
    "1. **Define Hypotheses & Variables**\n",
    "\n",
    "   * State null and alternative hypotheses explicitly.\n",
    "   * Identify independent, dependent, and control variables.\n",
    "\n",
    "2. **Use Proper Controls & Randomization**\n",
    "\n",
    "   * When possible, include both positive and negative controls.\n",
    "   * Randomly assign subjects or data batches to conditions to avoid bias.\n",
    "\n",
    "3. **Replicate & Report Variability**\n",
    "\n",
    "   * Run ‚â• 3 independent trials; report means ¬± standard deviations.\n",
    "   * If variation exceeds \\~5%, investigate sources before concluding.\n",
    "\n",
    "4. **Perform Power Analysis**\n",
    "\n",
    "   * Calculate required sample size up-front to detect your expected effect size (e.g., Cohen's d = 0.4).\n",
    "   * Document alpha (commonly 0.05) and desired power (commonly 0.8).\n",
    "\n",
    "## 2. üìä Comparison to Prior Work\n",
    "\n",
    "Authors often claim novelty without fair, head-to-head benchmarks against the most relevant baselines.\n",
    "\n",
    "**Best-Practice Solutions:**\n",
    "\n",
    "1. **Conduct a Focused Literature Review**\n",
    "\n",
    "   * Identify 3-5 seminal and recent papers tackling the same problem.\n",
    "   * Highlight gaps your approach fills.\n",
    "\n",
    "2. **Re-implement or Use Published Code**\n",
    "\n",
    "   * Whenever possible, obtain or re-implement baseline methods rather than quoting old results.\n",
    "   * Match preprocessing, hyperparameters, and evaluation metrics exactly.\n",
    "\n",
    "3. **Present Results in Comparative Tables**\n",
    "\n",
    "   ```markdown\n",
    "   | Method        | Dataset     | Metric     | Result         |\n",
    "   |---------------|-------------|------------|----------------|\n",
    "   | Proposed      | X           | Accuracy   | 87.4 %         |\n",
    "   | Baseline A    | X           | Accuracy   | 82.1 %         |\n",
    "   | Baseline B    | X           | Accuracy   | 84.3 %         |\n",
    "   ```\n",
    "\n",
    "   * Include confidence intervals or standard errors for each entry.\n",
    "\n",
    "4. **Discuss Trade-Offs**\n",
    "\n",
    "   * If your method improves one metric but worsens another (e.g., speed vs. accuracy), analyze why.\n",
    "   * Be transparent about limitations relative to baselines.\n",
    "\n",
    "## 3. üîç Quantitative Claims Need Proof\n",
    "\n",
    "‚Äú20 % improvement‚Äù sounds impressive‚Äîuntil readers realize there's no reported variance, p-value, or context for that number.\n",
    "\n",
    "**Best-Practice Solutions:**\n",
    "\n",
    "1. **State Claims Early & Precisely**\n",
    "\n",
    "   * In the Abstract/Introduction, quantify your main gain (e.g., ‚ÄúOur model reduces error by 20 %.‚Äù).\n",
    "\n",
    "2. **Prove Them in the Results Section**\n",
    "\n",
    "   * Show tabular or graphical results with error bars (95 % CI) or p-values.\n",
    "   * Example:\n",
    "\n",
    "     ```markdown\n",
    "     | Model         | Mean Error | 95 % CI      | p-value |\n",
    "     |---------------|------------|--------------|---------|\n",
    "     | Ours          | 0.12       | [0.10, 0.14] | 0.003   |\n",
    "     | Baseline      | 0.15       | [0.13, 0.17] | ‚Äî       |\n",
    "     ```\n",
    "\n",
    "3. **Use Appropriate Statistical Tests**\n",
    "\n",
    "   * T-tests, ANOVA, or non-parametric tests depending on distribution.\n",
    "   * Correct for multiple hypotheses when reporting several metrics.\n",
    "\n",
    "4. **Explain Practical Impact**\n",
    "\n",
    "   * Beyond percentages, describe what a 20 % reduction means in domain terms (e.g., 2 days saved in processing time).\n",
    "\n",
    "## 4. üìâ Dataset Size & Diversity\n",
    "\n",
    "Too-small or homogenous datasets lead to overfitting and findings that don't generalize.\n",
    "\n",
    "**Best-Practice Solutions:**\n",
    "\n",
    "1. **Follow ‚ÄúRule of Ten‚Äù When Feasible**\n",
    "\n",
    "   * Aim for ‚â• 10 samples per feature dimension (e.g., 50 features ‚Üí ‚â• 500 samples).\n",
    "\n",
    "2. **Leverage Power-Based Sample Size Calculations**\n",
    "\n",
    "   * For a medium effect (d = 0.4), two groups need \\~100 samples each.\n",
    "\n",
    "3. **Document Dataset Splits & Diversity**\n",
    "\n",
    "   * Report train/validation/test sizes and demographic or source diversity (e.g., multiple sites, time periods).\n",
    "\n",
    "4. **Augment When Necessary**\n",
    "\n",
    "   * If real-world data are limited, consider data augmentation, transfer learning, or synthetic data‚Äîalways flag these in methods.\n",
    "\n",
    "5. **Assess Generalizability**\n",
    "\n",
    "    * Perform sensitivity analyses: how do results change if you drop 10-20 % of data or vary domain characteristics?\n",
    "<|result|>\n",
    "And these are the concerns:\n",
    "<|concerns|>\n",
    "{final_result.read()}\n",
    "<|concerns|>\n",
    "Answer me with only the final guideline markdown.\n",
    "\"\"\"\n",
    "\n",
    "response = ollama.chat(model='qwen3:14b', messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": question\n",
    "    }\n",
    "])\n",
    "response = response['message']['content']\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3798fa1",
   "metadata": {},
   "source": [
    "## üîÆ Future work\n",
    "\n",
    "Additional work can be surely done, for example:\n",
    "\n",
    "- **Clustering & Embeddings**\n",
    "  Although I initially thought of TF‚ÄëIDF + K‚ÄëMeans approach for grouping extracted concerns, I haven‚Äôt yet integrated it and I preferred to go all-in with LLMs.\n",
    "\n",
    "- **Dynamic Context Windowing**  \n",
    "  The problem of mitigating prompt‚Äëlength issues remains, I may consider retrieval‚Äëaugmented prompting: store past concerns in a lightweight vector database, since they grow fast in case we analyze all the papers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
