{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42f400c1",
   "metadata": {},
   "source": [
    "# Assignment 3: Extracting ‚ÄúBetter‚ÄêPaper‚Äù Guidelines from the OpenReview Dataset\n",
    "\n",
    "This notebook documents the steps, decisions, and code used to leverage local LLMs for mining common reviewer feedback and producing a Markdown checklist of best practices.\n",
    "Lacking deep ML expertise, I relied heavily on Ollama‚Äîfirst with DeepSeek-8B, then (for better results) Qwen-3-14B to extract shared concerns from peer reviews. Finally, I augmented those LLM-generated bullets with Sentence-BERT embeddings and clustering to group similar feedback into coherent themes.\n",
    "But let‚Äôs start easily with an example of Qwen-3-14B extracting common concerns from the first three reviews in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f199ccfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, I need to help the user create markdown guidelines for authors based on the reviews they provided. Let me start by analyzing the three reviews to identify common issues and areas for improvement.\n",
      "\n",
      "First, Review 1 points out that the paper lacks rigorous experimentation. They didn't compare with earlier works like Joseph et al. and Wang et al., and didn't test on other domains like text classification. Also, there's a lack of theoretical analysis and some results are not conclusive. The abstract's claim about a 20.5% reduction in test error isn't addressed properly. They also mention issues with ablation studies and the need for more analysis on the warm-up phase and the selection pressure parameter.\n",
      "\n",
      "Review 2 mentions that the experiments don't show state-of-the-art results on standard datasets like MNIST, CIFAR-10, and CIFAR-100. The author is concerned about the method's effectiveness and suggests testing on larger datasets like Tiny ImageNet. There are also questions about the algorithm's convergence and theoretical guarantees. The review also points out that the entropy estimation becomes harder with more classes and asks if there are ways to mitigate that without increasing the window size. They also want more explanation on equation (4).\n",
      "\n",
      "Review 3 highlights that the method is heuristic and not well-justified theoretically. The experiments are limited to three datasets, which makes the results less conclusive. The introduction of new hyperparameters (like the annealing schedule) is noted, and the paper doesn't provide enough analysis on their impact. The reviewer would have given a stronger acceptance if there were more empirical or theoretical evidence.\n",
      "\n",
      "So, common mistakes from these reviews include:\n",
      "- Lack of thorough comparison with prior work.\n",
      "- Insufficient experimental rigor (e.g., limited datasets, no ablation studies).\n",
      "- Missing theoretical analysis.\n",
      "- Inadequate results discussion (e.g., not addressing claims, unclear results).\n",
      "- Poor explanation of key equations and hyperparameters.\n",
      "- Limited generalizability (only tested on image classification, not other domains).\n",
      "\n",
      "Now, the user wants a markdown guideline that addresses these points. The structure should have sections like Rigorous Experimentation, Comparison to Prior Work, Theoretical Justification, etc. Each section should list the common mistakes and best practices.\n",
      "\n",
      "I need to make sure each section is clear, uses the examples from the reviews, and provides actionable solutions. For instance, under Rigorous Experimentation, include comparing with relevant prior work, testing on diverse datasets, and conducting ablation studies. Under Theoretical Justification, mention the need for analysis of the method's properties and convergence guarantees.\n",
      "\n",
      "Also, the user mentioned including common mistakes and best solutions. Each section should have a heading, a brief description of the problem, and then bullet points with solutions. The markdown should be structured with headings, subheadings, and lists.\n",
      "\n",
      "I should check if the example provided by the user is followed. The example starts with # Paper Writing Guidelines, then sections with emojis and headings. Each section has a problem description and best practices as bullet points. I need to mirror that structure.\n",
      "\n",
      "Let me structure each section based on the identified issues. For example:\n",
      "\n",
      "## üß™ Rigorous Experimentation\n",
      "- Common mistakes: Not comparing with prior work, limited datasets, no ablation studies.\n",
      "- Solutions: Compare with relevant works, use diverse datasets, perform ablation studies.\n",
      "\n",
      "Similarly for other sections like Theoretical Justification, Results Interpretation, etc.\n",
      "\n",
      "I need to ensure that each part is concise and directly addresses the points from the reviews. Also, make sure that the markdown uses the correct syntax, like # for main headings, ## for subheadings, and bullet points with * or -.\n",
      "\n",
      "Finally, review the markdown to ensure it's comprehensive, covers all key areas from the reviews, and is easy to follow for authors looking to improve their paper writing.\n",
      "</think>\n",
      "\n",
      "# üìù Paper Writing Guidelines for Authors\n",
      "\n",
      "This guide outlines key principles to improve the quality and impact of your research papers, based on common issues identified in peer reviews.\n",
      "\n",
      "## üß™ Rigorous Experimentation  \n",
      "Many submissions fall short on experimental rigor‚Äîinsufficient controls, lack of repeatability, vague protocol descriptions, or no statistical power analysis.  \n",
      "\n",
      "**Best-Practice Solutions:**  \n",
      "1. **Compare with All Relevant Prior Work**  \n",
      "   * Include benchmarks from both recent and foundational works (e.g., Joseph et al., 2019; Wang et al., 2019).  \n",
      "   * Avoid cherry-picking competitors; evaluate against methods with similar goals.  \n",
      "2. **Test Across Diverse Domains and Tasks**  \n",
      "   * Validate methods on non-image tasks (e.g., text classification) to demonstrate generalizability.  \n",
      "   * Use large-scale datasets (e.g., Tiny ImageNet) to stress-test performance.  \n",
      "3. **Conduct Ablation Studies**  \n",
      "   * Analyze the impact of hyperparameters (e.g., selection pressure, warm-up phase).  \n",
      "   * Include baselines with and without key components (e.g., sliding window vs. growing window).  \n",
      "4. **Address Limitations in Results**  \n",
      "   * Clearly explain discrepancies (e.g., why online batch outperforms your method).  \n",
      "   * Justify claims (e.g., the 20.5% test error reduction) with detailed results.  \n",
      "\n",
      "## üìö Theoretical Justification  \n",
      "Weak theoretical analysis can undermine the novelty and impact of your method.  \n",
      "\n",
      "**Best-Practice Solutions:**  \n",
      "1. **Link to Existing Theories**  \n",
      "   * Connect your method to established frameworks (e.g., active learning, uncertainty quantification).  \n",
      "   * Analyze properties like expected model variance or convergence guarantees.  \n",
      "2. **Provide Intuition with Simpler Models**  \n",
      "   * Demonstrate the method‚Äôs validity on simple models (e.g., logistic regression).  \n",
      "   * Use visualizations (e.g., decision boundaries) to clarify intuition.  \n",
      "3. **Address Algorithmic Guarantees**  \n",
      "   * Prove convergence under certain conditions (e.g., stability of uncertainty estimates).  \n",
      "   * Discuss how hyperparameters (e.g., annealing schedules) affect behavior.  \n",
      "\n",
      "## üìä Results Interpretation  \n",
      "Vague or inconclusive results can weaken the paper‚Äôs contribution.  \n",
      "\n",
      "**Best-Practice Solutions:**  \n",
      "1. **Quantify and Contextualize Improvements**  \n",
      "   * Highlight statistical significance and task-specific benefits (e.g., \"20.5% reduction in test error on CIFAR-10\").  \n",
      "   * Compare results to baselines in both relative and absolute terms.  \n",
      "2. **Analyze Edge Cases and Anomalies**  \n",
      "   * Investigate unexpected patterns (e.g., sharp dips in loss curves).  \n",
      "   * Explain how results vary with task difficulty or dataset size.  \n",
      "3. **Use Clear and Consistent Metrics**  \n",
      "   * Avoid ambiguous labels (e.g., \"image classification\" vs. \"image classification from scratch\").  \n",
      "   * Define all acronyms and notation (e.g., $ P(y_i/x_i; q) $).  \n",
      "\n",
      "## üß† Method Design and Explanation  \n",
      "Unclear or under-motivated methods can confuse readers and reviewers.  \n",
      "\n",
      "**Best-Practice Solutions:**  \n",
      "1. **Clarify Key Design Choices**  \n",
      "   * Justify heuristic decisions (e.g., why a sliding window over a growing window?).  \n",
      "   * Explain hyperparameters (e.g., annealing schedules, window sizes).  \n",
      "2. **Simplify Complex Equations**  \n",
      "   * Break down formulas (e.g., equation (4)) with intuitive explanations.  \n",
      "   * Relate terms to known concepts (e.g., entropy, maximum likelihood).  \n",
      "3. **Highlight Novelty and Limitations**  \n",
      "   * Clearly state the method‚Äôs unique contributions (e.g., \"sliding window for recency bias\").  \n",
      "   * Acknowledge limitations (e.g., dependence on hyperparameters).  \n",
      "\n",
      "## üìÑ Writing and Presentation  \n",
      "Poorly written papers can obscure even strong results.  \n",
      "\n",
      "**Best-Practice Solutions:**  \n",
      "1. **Improve Clarity and Flow**  \n",
      "   * Use concise language and avoid jargon.  \n",
      "   * Structure sections to build a logical narrative (e.g., problem ‚Üí method ‚Üí results).  \n",
      "2. **Correct Technical Errors**  \n",
      "   * Proofread for typos (e.g., \"netowrks\" ‚Üí \"networks\").  \n",
      "   * Ensure consistency in notation and terminology.  \n",
      "3. **Visualize Key Insights**  \n",
      "   * Use graphs (e.g., loss convergence, entropy estimates) to support claims.  \n",
      "   * Label figures clearly and discuss their implications.  \n",
      "\n",
      "---  \n",
      "*Adhering to these guidelines can help you craft a more impactful, rigorous, and reader-friendly paper.*\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "question = r\"\"\"\n",
    "I am analizing a dataset of OpenReview papers because I want to create a small markdown of general guidelines, to help authors writing better papers.\n",
    "Here are three reviews of a paper I chose:\n",
    "<|review1|>\n",
    " This paper proposes Recency Bias, an adaptive mini batch selection method for training deep neural networks. To select informative minibatches for training, the proposed method maintains a fixed size sliding window of past model predictions for each data sample. At a given iteration, samples which have highly inconsistent predictions within the sliding window are added to the minibatch. The main contribution of this paper is the introduction of sliding window to remember past model predictions, as an improvement over the SOTA approach: Active Bias, which maintains a growing window of model predictions. Empirical studies are performed to show the superiority of Recency Bias over two SOTA approaches. Results are shown on the task of (1) image classification from scratch and (2) image classification by fine-tuning pretrained networks. +ves: + The idea of using a sliding window over a growing window in active batch selection is interesting. + Overall, the paper is well written. In particular, the Related Work section has a nice flow and puts the proposed method into context. Despite the method having limited novelty (sliding window instead of a growing window), the method has been well motivated by pointing out the limitations in SOTA methods. + The results section is well structured. It*s nice to see hyperparameter tuning results; and loss convergence graphs in various learning settings for each dataset. Concerns: - The key concern about the paper is the lack of rigorous experimentation to study the usefulness of the proposed method. Despite the paper stating that there have been earlier work (Joseph et al, 2019 and Wang et al, 2019) that attempt mini-batch selection, the paper does not compare with them. This is limiting. Further, since the proposed method is not specific to the domain of images, evaluating it on tasks other than image classification, such as text classification for instance, would have helped validate its applicability across domains. - Considering the limited results, a deeper analysis of the proposed method would have been nice. The idea of a sliding window over a growing window is a generic one, and there have been many efforts to theoretically analyze active learning over the last two decades. How does the proposed method fit in there? (For e.g., how does the expected model variance change in this setting?) Some form of theoretical/analytical reasoning behind the effectiveness of recency bias (which is missing) would provide greater insights to the community and facilitate further research in this direction. - The claim of 20.5% reduction in test error mentioned in the abstract has not been clearly addressed and pointed out in the results section of the paper. - On the same note, the results are not conclusively in favor of the proposed method, and only is marginally better than the competitors. Why does online batch perform consistently than the proposed method? There is no discussion of these inferences from the results. - The results would have been more complete if results were shown in a setting where just recency bias is used without the use of the selection pressure parameter. In other words, an ablation study on the effect of the selection pressure parameter would have been very useful. - How important is the warm-up phase to the proposed method? Considering the paper states that this is required to get good estimates of the quantization index of the samples, some ablation studies on reducing/increasing the warm-up phase and showing the results would have been useful to understand this. - Fig 4: Why are there sharp dips periodically in all the graphs? What do these correspond to? - The intuition behind the method is described well, however, the proposed method would have been really solidified if it were analysed in the context of a simple machine learning problem (such as logistic regression). As an example, verifying if the chosen minibatch samples are actually close to the decision boundary of a model (even if the model is very simple) would have helped analyze the proposed method well. Minor comments: * It would have been nice to see the relation between the effect of using recency bias and the difficulty of the task/dataset. * In the 2nd line in Introduction, it should be *deep networks* instead of *deep networks netowrks*. * Since both tasks in the experiments are about image classification, it would be a little misleading to present them as *image classification* and *finetuning*. A more informative way of titling them would be *image classification from scratch* and *image classification by finetuning*. * In Section 3.1, in the LHS of equation 3, it would be appropriate to use P(y_i/x_i; q) instead of P(y/x_i; q) since the former term was used in the paragraph. =====POST-REBUTTAL COMMENTS======== I thank the authors for the response and the efforts in the updated draft. Some of my queries were clarified. However, unfortunately, I still think more needs to be done to explain the consistency of the results and to study the generalizability of this work across datasets. I retain my original decision for these reasons.\n",
    "<|review1|>\n",
    "<|review2|>\n",
    "This paper proposes an interesting heuristic of batch construction from samples. Instead of the usual random sampling, the authors to sample based on some measures of the ``uncertainty‚Äù. To be specific, the uncertainty is measured as a normalized entropy estimated from a window of historical predictions. I like the idea of designing more sophisticated ways to encourage more exploration over the samples that the model is not good at. The thought is similar as active learning. It is interesting to see how similar thought can be used to improve the performance of the algorithm in the general batch gradient descent setting. On the other hand I am not quite convinced the proposed way is truly better. The main concern is the experiments do not quite show the state-of-the-art result at all. It is not even close on MNIST, CIFAR-10 and CIFAR-100. Also those datasets are relatively small one. Can authors add results on larger datasets such as tiny image net? Besides this main concern I also have some worries about the design of the algorithm. I listed them below: 1. The vanilla stochastic gradient descent can be roughly justified since the expectation of the stochastic gradient is the true gradient of the loss. Now with the proposed heuristic will this still be true? 2. Is there any guarantee the algorithm can converge? It is not clear to me as the optimization proceeds the ``uncertainty‚Äù may oscillate. Is there any condition when the convergence is guaranteed? 3. As the number of classes grows the estimation of the entropy itself is a tough problem. Is there any way to mitigate this issue other than increase the window size? Another minor comment: Could the authors add more explanation on equation (4)? For example, is related to the maximum entropy led by a uniform distribution, and the summation term in (4) is related to the empirical entropy.\n",
    "<|review2|>\n",
    "<|review3|>\n",
    "This paper explores a well motivated but very heuristic idea for selecting the next samples to train on for training deep learning models. This method relies on looking at the uncertainty of predictions of in the recent history of statements and preferring those instances that have a predictive uncertainty over the recent predictions. This allows the training method to train on instances that are neither too hard nor too easy and focus on reducing the uncertainty whenever it has the greatest potential gain to do so. There are two extra components that make this method work: - Windowing: only looking at the recent history of the instances which has two effects: firstly, the current state of the model is explored which gives a more recent assessment relative to the current state of the model. Secondly, it makes the algorithm faster by reducing the overhead of analyzing the prediction history of samples. - Annealing the selection bias: as the training goes on the selection becomes more random and less biased. This approach is evaluated in on three simple data-sets: MNIST, CIFAR-10 and CIFAR-100. Although this is a very limited subset of models, the results are consistent and statistically significant, although their effect is not really huge. The paper gives very little theoretical justification or analysis of the results but gives only the presented empirical evidence which seems to support the hypothesis on the efficacy of the approach. Another drawback of the approach is that it introduces new hyperparameters: those governing the annealing schedule for the selection bias. Since the approach seems efficient in a relatively constrained setup, it can be reasonably expected that it might be helpful in more general situations, therefore. On the other hand, since it is only evaluated on three very similar tasks, it limits the conclusiveness of the results. That*s why I would for weak accept. In the presence of more empirical (or even theoretical) evidence, I would vote for strong accept.\n",
    "<|review3|>\n",
    "Can you help me creating the markdown guidelines, inserting common mistakes that are worth citing and the best solution to fix them?\n",
    "I want something like:\n",
    "<|result|>\n",
    "# üìù Paper Writing Guidelines for Authors\n",
    "\n",
    "This guide outlines key principles to ...\n",
    "\n",
    "## üß™ Rigorous Experimentation\n",
    "\n",
    "Many submissions fall short on experimental rigor‚Äîinsufficient controls, lack of repeatability, vague protocol descriptions, or no statistical power analysis.\n",
    "\n",
    "**Best-Practice Solutions:**\n",
    "\n",
    "1. **Define Hypotheses & Variables**\n",
    "    * State null and alternative hypotheses explicitly.\n",
    "\n",
    "...\n",
    "\n",
    "## 2. üìä Comparison to Prior Work\n",
    "<|result|>\n",
    "\n",
    "Answer me with only the final guideline markdown.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "response = ollama.chat(model='qwen3:14b', messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": question\n",
    "    }\n",
    "])\n",
    "\n",
    "response = response['message']['content']\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c559f803",
   "metadata": {},
   "source": [
    "## üí° The initial idea\n",
    "At this point, the initial idea was to go through the reviews in batches, extract the main criticisms, and add them to a text file (already filled with some tips to guide the LLM) collecting all the feedback gathered so far. To do this, I would prompt the LLM to expand the markdown document if it identified any new suggestions. I thought also to maintain a list of best solutions that could be adopted to fix the concerns.\n",
    "\n",
    "Here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6cb3bb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# üìù Paper Writing Guidelines for Authors  \n",
      "\n",
      "This guide outlines key principles to help authors avoid common pitfalls and meet the expectations of peer reviewers and the broader research community.  \n",
      "\n",
      "## üìä Comparison to Prior Work  \n",
      "\n",
      "Authors often claim novelty without fair, head-to-head benchmarks against the most relevant baselines.  \n",
      "\n",
      "### Best-Practice Solutions  \n",
      "\n",
      "1. **Conduct a Focused Literature Review**  \n",
      "   - Identify 3-5 seminal and recent papers tackling the same problem.  \n",
      "   - Highlight gaps your approach fills.  \n",
      "\n",
      "2. **Re-implement or Use Published Code**  \n",
      "   - Whenever possible, obtain or re-implement baseline methods rather than quoting old results.  \n",
      "   - Match preprocessing, hyperparameters, and evaluation metrics exactly.  \n",
      "\n",
      "3. **Present Results in Comparative Tables**  \n",
      "   ```markdown\n",
      "   | Method        | Dataset     | Metric     | Result         |\n",
      "   |---------------|-------------|------------|----------------|\n",
      "   | Proposed      | X           | Accuracy   | 87.4 %         |\n",
      "   | Baseline A    | X           | Accuracy   | 82.1 %         |\n",
      "   | Baseline B    | X           | Accuracy   | 84.3 %         |\n",
      "   ```  \n",
      "   - Include confidence intervals or standard errors for each entry.  \n",
      "\n",
      "4. **Discuss Trade-Offs**  \n",
      "   - If your method improves one metric but worsens another (e.g., speed vs. accuracy), analyze why.  \n",
      "   - Be transparent about limitations relative to baselines.  \n",
      "\n",
      "## üìö Other Key Sections (if applicable)  \n",
      "\n",
      "If additional sections (e.g., \"Methodology,\" \"Results,\" \"Conclusion\") exist, they should follow the same structure:  \n",
      "- Use emojis in all headings.  \n",
      "- Avoid third-level subheadings.  \n",
      "- Merge similar sections into a single heading if needed (e.g., combine \"Results\" and \"Discussion\" into \"Findings and Implications\").  \n",
      "\n",
      "---  \n",
      "**Note:** Ensure all headings are consistent in formatting, and use emojis to enhance readability without overcomplicating the structure.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import openpyxl\n",
    "from io import SEEK_END\n",
    "\n",
    "# this file contains the template result we want to get from the model\n",
    "starting_result = open(\"expected_result.txt\", \"r\")\n",
    "final_result = open(\"first_idea_result.md\", \"w+\")\n",
    "final_result.write(starting_result.read())\n",
    "final_result.seek(0, SEEK_END)\n",
    "starting_result.close()\n",
    "\n",
    "def get_main_question(result, review1, review2, review3):\n",
    "    return f\"\"\"I am analizing a dataset of OpenReview papers because I want to create a small markdown of general guidelines, to help authors writing better papers.\n",
    "So I collected some reviews and this is the result I created:\n",
    "<|result|>\n",
    "{result}\n",
    "<|result|>\n",
    "Now I have three reviews of a paper I chose.\n",
    "I want you to find any new common mistakes that are worth citing and add them as a new heading to the markdown I provided you. Mantain the original style of the markdown (with headings, emoji in the headings, no headings of third level, etc.). If the mistakes are already present, do not add them. If you don't find any new mistake, leave the markdown as before.\n",
    "Here are the reviews:\n",
    "<|review1|>\n",
    "{review1}\n",
    "<|review1|>\n",
    "<|review2|>\n",
    "{review2}\n",
    "<|review2|>\n",
    "<|review3|>\n",
    "{review3}\n",
    "<|review3|>\n",
    "Answer me with only the final guideline markdown.\n",
    "\"\"\"\n",
    "\n",
    "sheet = openpyxl.load_workbook(\"or_2020.xlsx\").active\n",
    "\n",
    "for row in range(5, 40, 3): # there should be sheet.max_row, but with my pc would take ~80 hours\n",
    "    review1 = sheet.cell(row, 6).value\n",
    "    review2 = sheet.cell(row+1, 6).value\n",
    "    review3 = sheet.cell(row+2, 6).value\n",
    "    final_question = get_main_question(final_result.read(), review1, review2, review3)\n",
    "    final_result.seek(0, SEEK_END)\n",
    "    response = ollama.chat(model='qwen3:14b', messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": final_question\n",
    "        }\n",
    "    ])\n",
    "    response = response['message']['content']\n",
    "    if response.find('```markdown') != -1:\n",
    "        response = response.split('```markdown')[1].split('```')[0] # get the guideline markdown part\n",
    "    elif response.find('#') != -1:\n",
    "        response = response.split('#')[1]\n",
    "    final_result.write(response)\n",
    "\n",
    "final_result.seek(0)\n",
    "uniform_question = f\"\"\"\n",
    "Uniform this markdown to have a uniform style (for example every heading have an emoji, no subheading of third level, start with the title \"# üìù Paper Writing Guidelines for Authors\", etc). Finally, if there are headings that are too similar uniform them in a single heading.\n",
    "Here's the markdown:\n",
    "<|markdown|>\n",
    "{final_result.read()}\n",
    "<|markdown|>\n",
    "\"\"\"\n",
    "response = ollama.chat(model='qwen3:14b', messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": uniform_question\n",
    "    }\n",
    "])\n",
    "response = response['message']['content']\n",
    "# Strip off any '<‚Äã/think>' section if present\n",
    "if '</think>' in response:\n",
    "\tresponse = response.split('</think>', 1)[1]\n",
    "final_result.write(response)\n",
    "print(response)\n",
    "final_result.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0ade20",
   "metadata": {},
   "source": [
    "The result turned out to be not too bad, but some problems arised:\n",
    "\n",
    "- **Prompt‚Äêlength explosion:** as the markdown grows, the LLM allucinates or cuts off.\n",
    "- **Inconsistency:** later generations sometimes rephrase or lose earlier entries.\n",
    "- **Throughput:** re‚Äësending the entire file for each triple becomes slow.\n",
    "\n",
    "For this reason, I decided to revise the strategy.\n",
    "\n",
    "## üîÑ Revised Strategy: Use LLMs + Clustering\n",
    "\n",
    "Rather than carrying forward the cumulative output and feeding it back to the LLM each time, I decided to revamp the workflow to separate concern extraction from guideline synthesis, improving both speed and flexibility. Here‚Äôs how it works in practice:\n",
    "\n",
    "1. **Dynamic Review Grouping:**  \n",
    "   We scan the spreadsheet row by row, grouping all consecutive reviews for the same paper title into a single batch prompt.\n",
    "\n",
    "2. **One-Shot LLM Extraction:**  \n",
    "   We send all collected reviews with a clear instruction:  \n",
    "   > ‚ÄúList only the common concerns shared by these reviews as bullet points.‚Äù  \n",
    "   The LLM returns clean, newline-delimited bullets, which we parse and trim.\n",
    "\n",
    "3. **Checkpointing in JSON:**\n",
    "\tBy saving title ‚Üí concerns to a JSON file, we can interrupt and resume processing without redoing work.  \n",
    "\n",
    "4. **Normalization & Deduplication:**\n",
    "\tOnce all concerns are collected, we lowercase, strip punctuation, and filter out noise before clustering.\n",
    "\n",
    "5. **Embeddings & Clustering** \n",
    "   Cleaned sentences are embedded with Sentence-BERT and automatically grouped (e.g. hierarchical or K-Means). For each cluster, we pick a representative sentence‚Äîeither the one closest to the centroid or simply the longest‚Äîand assemble a preliminary Markdown checklist.\n",
    "\n",
    "Finally, a quick human pass lets us assign theme labels (e.g., Novelty, Baselines, Theory) and polish the bullets into ‚ÄúDo‚Äù/‚ÄúDon‚Äôt‚Äù guidelines.\n",
    "\n",
    "Here's the code of points 1, 2 and 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc768f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 'Prestopping: How Does Early Stopping Help Generalization Against Label Noise? | OpenReview', already in JSON.\n",
      "Skipping 'Analysis and Interpretation of Deep CNN Representations as Perceptual Quality Features | OpenReview', already in JSON.\n",
      "Skipping 'Improving Evolutionary Strategies with Generative Neural Networks | OpenReview', already in JSON.\n",
      "Skipping 'Wide Neural Networks are Interpolating Kernel Methods: Impact of Initialization on Generalization | OpenReview', already in JSON.\n",
      "Skipping 'SSE-PT: Sequential Recommendation Via Personalized Transformer | OpenReview', already in JSON.\n",
      "Skipping 'Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization | OpenReview', already in JSON.\n",
      "Skipping 'Count-guided Weakly Supervised Localization Based on Density Map | OpenReview', already in JSON.\n",
      "Skipping 'Star-Convexity in Non-Negative Matrix Factorization | OpenReview', already in JSON.\n",
      "Skipping 'Noise Regularization for Conditional Density Estimation | OpenReview', already in JSON.\n",
      "Skipping 'Amharic Negation Handling | OpenReview', already in JSON.\n",
      "Skipping 'Neural Maximum Common Subgraph Detection with Guided Subgraph Extraction | OpenReview', already in JSON.\n",
      "Skipping 'Context-aware Attention Model for Coreference Resolution | OpenReview', already in JSON.\n",
      "Skipping 'When Does Self-supervision Improve Few-shot Learning? | OpenReview', already in JSON.\n",
      "Skipping 'Deep Reasoning Networks: Thinking Fast and Slow, for Pattern De-mixing | OpenReview', already in JSON.\n",
      "Skipping 'HOW IMPORTANT ARE NETWORK WEIGHTS? TO WHAT EXTENT DO THEY NEED AN UPDATE? | OpenReview', already in JSON.\n",
      "Skipping 'Stochastically Controlled Compositional Gradient for the Composition problem | OpenReview', already in JSON.\n",
      "Skipping 'Probabilistic modeling the hidden layers of deep neural networks | OpenReview', already in JSON.\n",
      "Skipping 'On the Decision Boundaries of Deep Neural Networks: A Tropical Geometry Perspective | OpenReview', already in JSON.\n",
      "Skipping 'Accelerated Information Gradient flow | OpenReview', already in JSON.\n",
      "Skipping 'EgoMap: Projective mapping and structured egocentric memory for Deep RL | OpenReview', already in JSON.\n",
      "Skipping 'Neural Linear Bandits: Overcoming Catastrophic Forgetting through Likelihood Matching | OpenReview', already in JSON.\n",
      "Skipping 'AutoSlim: Towards One-Shot Architecture Search for Channel Numbers | OpenReview', already in JSON.\n",
      "Skipping 'Scaling Up Neural Architecture Search with Big Single-Stage Models | OpenReview', already in JSON.\n",
      "Skipping 'Semantics Preserving Adversarial Attacks | OpenReview', already in JSON.\n",
      "Skipping 'Out-of-Distribution Detection Using Layerwise Uncertainty in Deep Neural Networks | OpenReview', already in JSON.\n",
      "Skipping 'Explaining A Black-box By Using A Deep Variational Information Bottleneck Approach | OpenReview', already in JSON.\n",
      "Skipping 'ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE | OpenReview', already in JSON.\n",
      "Skipping 'Dimensional Reweighting Graph Convolution Networks | OpenReview', already in JSON.\n",
      "Skipping 'VIDEO AFFECTIVE IMPACT PREDICTION WITH MULTIMODAL FUSION AND LONG-SHORT TEMPORAL CONTEXT | OpenReview', already in JSON.\n",
      "Skipping 'Visual Imitation with Reinforcement Learning using Recurrent Siamese Networks | OpenReview', already in JSON.\n",
      "Skipping 'Label Cleaning with Likelihood Ratio Test | OpenReview', already in JSON.\n",
      "Skipping 'Data augmentation instead of explicit regularization | OpenReview', already in JSON.\n",
      "Skipping 'Support-guided Adversarial Imitation Learning | OpenReview', already in JSON.\n",
      "Skipping 'Low Bias Gradient Estimates for Very Deep Boolean Stochastic Networks | OpenReview', already in JSON.\n",
      "Skipping 'X-Forest: Approximate Random Projection Trees for Similarity Measurement | OpenReview', already in JSON.\n",
      "Skipping 'Learning Reusable Options for Multi-Task Reinforcement Learning | OpenReview', already in JSON.\n",
      "Skipping 'TechKG: A Large-Scale Chinese Technology-Oriented Knowledge Graph | OpenReview', already in JSON.\n",
      "Skipping 'Effective Mechanism to Mitigate Injuries During NFL Plays | OpenReview', already in JSON.\n",
      "Skipping 'Stabilizing DARTS with Amended Gradient Estimation on Architectural Parameters | OpenReview', already in JSON.\n",
      "Skipping 'Utilizing Edge Features in Graph Neural Networks via Variational Information Maximization | OpenReview', already in JSON.\n",
      "Skipping 'Learning Structured Communication for Multi-agent Reinforcement Learning | OpenReview', already in JSON.\n",
      "Skipping 'Towards Stable and comprehensive Domain Alignment: Max-Margin Domain-Adversarial Training | OpenReview', already in JSON.\n",
      "Skipping 'Group-Connected Multilayer Perceptron Networks | OpenReview', already in JSON.\n",
      "Skipping 'Regional based query in graph active learning | OpenReview', already in JSON.\n",
      "Skipping 'RL-LIM: Reinforcement Learning-based Locally Interpretable Modeling | OpenReview', already in JSON.\n",
      "Skipping 'Data Valuation using Reinforcement Learning | OpenReview', already in JSON.\n",
      "Skipping 'Equivariant neural networks and equivarification | OpenReview', already in JSON.\n",
      "Skipping 'Unifying Graph Convolutional Neural Networks and Label Propagation | OpenReview', already in JSON.\n",
      "Processing 'Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning | OpenReview' with 3 reviews.\n",
      "['- Insufficient emphasis on novelty and lack of clear articulation of contribution.', '- Overlap with prior work (e.g., Fitted Q-iteration by Advantage Weighted Regression), leading to incremental contributions.', '- Weak experimental results: failure to outperform existing methods on standard benchmarks (e.g., MuJoCo tasks).', '- Inadequate benchmark selection (e.g., using uncommon tasks like LunarLander instead of standard ones).', '- Insufficient experimental rigor (e.g., using only 5 seeds for MuJoCo experiments).', '- Lack of theoretical or empirical analysis of key components (e.g., value function estimation stability).', '- Missing citations to closely related work, weakening the novelty claim.', '- Ambiguities in equations, algorithm descriptions, and code implementation details.']\n",
      "Collected 8 concerns for 'Advantage Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning | OpenReview'.\n",
      "Processing 'Cascade Style Transfer | OpenReview' with 3 reviews.\n",
      "['- Lack of Novelty: The proposed methods are criticized as simple combinations of existing techniques without addressing fundamental challenges in style transfer.', '- Insufficient Experimental Analysis: Limited exploration of hyperparameters, lack of systematic studies on the effect of combining multiple methods, and minimal quantitative evaluation.', '- Missing Literature Review: The paper fails to thoroughly compare with prior work on mixing styles or address key references in the field.', '- Reliance on Qualitative Results: Overemphasis on qualitative comparisons without robust quantitative analysis to validate claims of improvement.', '- Incomplete/Unconvincing User Studies: Poorly designed or inadequately reported human preference studies, with missing details on methodology, statistical significance, and bias control.']\n",
      "Collected 5 concerns for 'Cascade Style Transfer | OpenReview'.\n",
      "Processing 'FoveaBox: Beyound Anchor-based Object Detection | OpenReview' with 3 reviews.\n",
      "['- Lack of clear differentiation from prior work (e.g., similarity to CenterNet, FCOS, DeepMask).', '- Insufficient comparison with existing methods in terms of performance, speed, and hyperparameters.', '- Limited experimental analysis (e.g., ablation studies on other datasets, computational cost).', '- Discretization of scale still resembling anchor-based approaches.', '- Ambiguity in addressing overlapping bounding boxes and inference efficiency.', '- Writing clarity and technical explanation issues (e.g., unclear figures, confusing descriptions).']\n",
      "Collected 6 concerns for 'FoveaBox: Beyound Anchor-based Object Detection | OpenReview'.\n",
      "Processing 'Diving into Optimization of Topology in Neural Networks | OpenReview' with 4 reviews.\n",
      "['- Lack of theoretical analysis and depth in experiments', '- Small or unclear empirical improvements in accuracy', '- Need for reporting variance across multiple experimental runs', '- Insufficient comparison to prior work (e.g., DARTS, [4], Xie et al.)', '- Terminology issues (e.g., \"searching space\" vs. \"search space,\" \"topology\" vs. \"soft topology\")', '- Ambiguity in methodology and setup (e.g., Table 3, sparsity regularization)', '- Lack of clarity in explanations and figures', '- Limited novelty compared to existing methods (e.g., DARTS, MaskConnect)', '- Missing related work in neural architecture search (NAS) context', '- Concerns about the definition and discretization of \"topology\"', '- Inadequate explanation of baselines and their differences from prior approaches']\n",
      "Collected 11 concerns for 'Diving into Optimization of Topology in Neural Networks | OpenReview'.\n",
      "Processing 'Learning to Defense by Learning to Attack | OpenReview' with 3 reviews.\n",
      "[\"- Theoretical Justification: Lack of sufficient theoretical analysis or guarantees (e.g., certified robustness) to support the method's effectiveness.\", '- Comparison to State-of-the-Art: Insufficient comparison with existing methods (e.g., TRADES, Carlini‚Äôs work) or evaluation under different threat models (e.g., L2, unrestricted attacks).', '- Evaluation Rigor: Need for more thorough evaluation, including adaptive attack testing and validation of claims (e.g., the \"limiting cycle\" argument in Figure 1).', '- Training and Generalization: Limited discussion on training challenges, heuristics, or how the framework generalizes beyond the tested scenarios (e.g., CIFAR-10/100).', '- Practical Utility: Concerns about the framework‚Äôs practicality, including the effectiveness of the attacker network and its impact on defense performance.']\n",
      "Collected 5 concerns for 'Learning to Defense by Learning to Attack | OpenReview'.\n",
      "Processing 'Unsupervised Learning of Node Embeddings by Detecting Communities | OpenReview' with 3 reviews.\n",
      "[\"- Lack of Clarity in Algorithm Description: The paper fails to clearly summarize the complete algorithm, including input, output, and parameters, leading to confusion about the method's structure.\", '- Insufficient Comparison to Prior Work: The paper does not adequately compare with relevant baselines (e.g., community-preserving node embedding methods or existing spectral approaches), reducing the perceived novelty.', '- Inconsistent or Incomplete Empirical Evaluation: Results are compared using different measures across tables, and experiments lack scalability/efficiency analysis on large graphs or comparisons with alternative two-step methods.', '- Limited Novelty: The approach is seen as combining existing techniques without sufficient differentiation from prior work (e.g., methods that transfer matrix E to H to P).', \"- Ambiguity in Theoretical Justification: The paper lacks detailed explanation of the learning process (e.g., how the node embedding matrix E is obtained) and deeper analysis of the method's theoretical underpinnings.\", '- Inadequate Analysis of Results: Trends in experimental results (e.g., increasing/decreasing performance with batch size) are not thoroughly explained, and the correlation between detected communities and original labels is not investigated.']\n",
      "Collected 6 concerns for 'Unsupervised Learning of Node Embeddings by Detecting Communities | OpenReview'.\n",
      "Processing 'Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization | OpenReview' with 3 reviews.\n",
      "['- Lack of comparison to established baselines (e.g., BERTSUM, DUC datasets, GPT-2).', '- Insufficient evaluation on diverse or human-annotated datasets (e.g., DUC, real-world summarization tasks).', '- Missing ablation studies to analyze the contribution of key components (e.g., lead bias, filtering criteria).', '- No human evaluation to validate the quality of generated summaries.', '- Inconsistent or unclear experimental setup (e.g., varying hyperparameters, metrics, data filtering criteria).', '- Lack of statistical significance analysis for reported results.', '- Concerns about novelty, with claims of non-unique ideas (e.g., lead bias in datasets).', '- Ambiguous or incomplete explanations in the paper (e.g., unclear methodology, unclear sentences in sections).']\n",
      "Collected 8 concerns for 'Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization | OpenReview'.\n",
      "Processing 'Deep Audio Prior | OpenReview' with 3 reviews.\n",
      "['- Lack of detailed model architecture and training process descriptions', '- Insufficient experimental details (e.g., hyperparameter tuning, evaluation protocols, statistical analysis)', '- Limited dataset diversity and lack of comparison to standard benchmarks', '- Ambiguous or missing theoretical/motivational justification for method choices', '- Poor presentation clarity (e.g., typos, unclear figures, inconsistent equations)', '- Inadequate explanation of key components (e.g., loss function behavior, source generation)', '- Missing ablation studies or analysis of critical parameters (e.g., annealing schedules, window sizes)', '- Overreliance on qualitative results without quantitative validation', '- Lack of discussion on generalizability across tasks or domains', '- Incomplete or vague descriptions of algorithms and their potential edge cases']\n",
      "Collected 10 concerns for 'Deep Audio Prior | OpenReview'.\n",
      "\n",
      "‚úÖ Saved 57 papers' concerns to concerns.json\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import openpyxl\n",
    "import os\n",
    "import json\n",
    "\n",
    "OUT_PATH = \"concerns.json\"\n",
    "SHEET_PATH = \"or_2020.xlsx\"\n",
    "\n",
    "# Load the OpenReview sheet\n",
    "sheet = openpyxl.load_workbook(SHEET_PATH).active\n",
    "\n",
    "# Load existing results if any\n",
    "if os.path.exists(OUT_PATH):\n",
    "    with open(OUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        results = json.load(f)\n",
    "else:\n",
    "    results = {}\n",
    "\n",
    "def get_main_question_dynamic(reviews):\n",
    "    \"\"\"\n",
    "    Build the prompt for an arbitrary number of reviews.\n",
    "    reviews: list of strings, each representing a review.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "    for i, rev in enumerate(reviews, start=1):\n",
    "        sections.append(f\"<|review{i}|>\\n{rev}\\n<|review{i}|>\")\n",
    "    reviews_block = \"\\n\".join(sections)\n",
    "    return (\n",
    "        \"Perfect, now I have other reviews of a paper I chose:\\n\"\n",
    "        f\"{reviews_block}\\n\"\n",
    "        \"Answer me with only the common concerns that the reviewers have, \"\n",
    "        \"with only the bullet points without any other explanation.\"\n",
    "    )\n",
    "\n",
    "question = r\"\"\"\n",
    "I am analizing a dataset of OpenReview papers with reviews of the papers because I want to extract some common concern that the reviewers have, to help authors know in advance what are the common concerns.\n",
    "In particular, I have three reviews of a paper I chose:\n",
    "<|review1|>\n",
    " This paper proposes Recency Bias, an adaptive mini batch selection method for training deep neural networks. To select informative minibatches for training, the proposed method maintains a fixed size sliding window of past model predictions for each data sample. At a given iteration, samples which have highly inconsistent predictions within the sliding window are added to the minibatch. The main contribution of this paper is the introduction of sliding window to remember past model predictions, as an improvement over the SOTA approach: Active Bias, which maintains a growing window of model predictions. Empirical studies are performed to show the superiority of Recency Bias over two SOTA approaches. Results are shown on the task of (1) image classification from scratch and (2) image classification by fine-tuning pretrained networks. +ves: + The idea of using a sliding window over a growing window in active batch selection is interesting. + Overall, the paper is well written. In particular, the Related Work section has a nice flow and puts the proposed method into context. Despite the method having limited novelty (sliding window instead of a growing window), the method has been well motivated by pointing out the limitations in SOTA methods. + The results section is well structured. It*s nice to see hyperparameter tuning results; and loss convergence graphs in various learning settings for each dataset. Concerns: - The key concern about the paper is the lack of rigorous experimentation to study the usefulness of the proposed method. Despite the paper stating that there have been earlier work (Joseph et al, 2019 and Wang et al, 2019) that attempt mini-batch selection, the paper does not compare with them. This is limiting. Further, since the proposed method is not specific to the domain of images, evaluating it on tasks other than image classification, such as text classification for instance, would have helped validate its applicability across domains. - Considering the limited results, a deeper analysis of the proposed method would have been nice. The idea of a sliding window over a growing window is a generic one, and there have been many efforts to theoretically analyze active learning over the last two decades. How does the proposed method fit in there? (For e.g., how does the expected model variance change in this setting?) Some form of theoretical/analytical reasoning behind the effectiveness of recency bias (which is missing) would provide greater insights to the community and facilitate further research in this direction. - The claim of 20.5% reduction in test error mentioned in the abstract has not been clearly addressed and pointed out in the results section of the paper. - On the same note, the results are not conclusively in favor of the proposed method, and only is marginally better than the competitors. Why does online batch perform consistently than the proposed method? There is no discussion of these inferences from the results. - The results would have been more complete if results were shown in a setting where just recency bias is used without the use of the selection pressure parameter. In other words, an ablation study on the effect of the selection pressure parameter would have been very useful. - How important is the warm-up phase to the proposed method? Considering the paper states that this is required to get good estimates of the quantization index of the samples, some ablation studies on reducing/increasing the warm-up phase and showing the results would have been useful to understand this. - Fig 4: Why are there sharp dips periodically in all the graphs? What do these correspond to? - The intuition behind the method is described well, however, the proposed method would have been really solidified if it were analysed in the context of a simple machine learning problem (such as logistic regression). As an example, verifying if the chosen minibatch samples are actually close to the decision boundary of a model (even if the model is very simple) would have helped analyze the proposed method well. Minor comments: * It would have been nice to see the relation between the effect of using recency bias and the difficulty of the task/dataset. * In the 2nd line in Introduction, it should be *deep networks* instead of *deep networks netowrks*. * Since both tasks in the experiments are about image classification, it would be a little misleading to present them as *image classification* and *finetuning*. A more informative way of titling them would be *image classification from scratch* and *image classification by finetuning*. * In Section 3.1, in the LHS of equation 3, it would be appropriate to use P(y_i/x_i; q) instead of P(y/x_i; q) since the former term was used in the paragraph. =====POST-REBUTTAL COMMENTS======== I thank the authors for the response and the efforts in the updated draft. Some of my queries were clarified. However, unfortunately, I still think more needs to be done to explain the consistency of the results and to study the generalizability of this work across datasets. I retain my original decision for these reasons.\n",
    "<|review1|>\n",
    "<|review2|>\n",
    "This paper proposes an interesting heuristic of batch construction from samples. Instead of the usual random sampling, the authors to sample based on some measures of the ``uncertainty‚Äù. To be specific, the uncertainty is measured as a normalized entropy estimated from a window of historical predictions. I like the idea of designing more sophisticated ways to encourage more exploration over the samples that the model is not good at. The thought is similar as active learning. It is interesting to see how similar thought can be used to improve the performance of the algorithm in the general batch gradient descent setting. On the other hand I am not quite convinced the proposed way is truly better. The main concern is the experiments do not quite show the state-of-the-art result at all. It is not even close on MNIST, CIFAR-10 and CIFAR-100. Also those datasets are relatively small one. Can authors add results on larger datasets such as tiny image net? Besides this main concern I also have some worries about the design of the algorithm. I listed them below: 1. The vanilla stochastic gradient descent can be roughly justified since the expectation of the stochastic gradient is the true gradient of the loss. Now with the proposed heuristic will this still be true? 2. Is there any guarantee the algorithm can converge? It is not clear to me as the optimization proceeds the ``uncertainty‚Äù may oscillate. Is there any condition when the convergence is guaranteed? 3. As the number of classes grows the estimation of the entropy itself is a tough problem. Is there any way to mitigate this issue other than increase the window size? Another minor comment: Could the authors add more explanation on equation (4)? For example, is related to the maximum entropy led by a uniform distribution, and the summation term in (4) is related to the empirical entropy.\n",
    "<|review2|>\n",
    "<|review3|>\n",
    "This paper explores a well motivated but very heuristic idea for selecting the next samples to train on for training deep learning models. This method relies on looking at the uncertainty of predictions of in the recent history of statements and preferring those instances that have a predictive uncertainty over the recent predictions. This allows the training method to train on instances that are neither too hard nor too easy and focus on reducing the uncertainty whenever it has the greatest potential gain to do so. There are two extra components that make this method work: - Windowing: only looking at the recent history of the instances which has two effects: firstly, the current state of the model is explored which gives a more recent assessment relative to the current state of the model. Secondly, it makes the algorithm faster by reducing the overhead of analyzing the prediction history of samples. - Annealing the selection bias: as the training goes on the selection becomes more random and less biased. This approach is evaluated in on three simple data-sets: MNIST, CIFAR-10 and CIFAR-100. Although this is a very limited subset of models, the results are consistent and statistically significant, although their effect is not really huge. The paper gives very little theoretical justification or analysis of the results but gives only the presented empirical evidence which seems to support the hypothesis on the efficacy of the approach. Another drawback of the approach is that it introduces new hyperparameters: those governing the annealing schedule for the selection bias. Since the approach seems efficient in a relatively constrained setup, it can be reasonably expected that it might be helpful in more general situations, therefore. On the other hand, since it is only evaluated on three very similar tasks, it limits the conclusiveness of the results. That*s why I would for weak accept. In the presence of more empirical (or even theoretical) evidence, I would vote for strong accept.\n",
    "<|review3|>\n",
    "Answer me with only the common concern that the reviewers have, with only the bullet points without any other explanation.\n",
    "\"\"\"\n",
    "\n",
    "answer = r\"\"\"\n",
    "- Rigorous Experimentation: Many submissions fall short on experimental rigor‚Äîinsufficient controls, lack of repeatability, vague protocol descriptions, or no statistical power analysis.\n",
    "- Comparison to Prior Work: Authors often claim novelty without fair, head-to-head benchmarks against the most relevant baselines.\n",
    "- Quantitative Claims Need Proof: Percentages are meaningless without a clear definition of the metric, baseline, and statistical significance. For example, \"Our method is 20% better\" is vague without context.\n",
    "- Dataset Size & Diversity: Too-small or homogenous datasets lead to overfitting and findings that don't generalize.\n",
    "\"\"\"\n",
    "\n",
    "# First row to process is 5, since the first 3 rows have been used to collect the starting result\n",
    "row = 5\n",
    "max_row = sheet.max_row\n",
    "\n",
    "while row <= max_row:\n",
    "    title = sheet.cell(row, 1).value\n",
    "    \n",
    "\t# Skip if already processed\n",
    "    if title in results:\n",
    "        print(f\"Skipping {title!r}, already in JSON.\")\n",
    "        # Skip to the next title\n",
    "        while row <= max_row and sheet.cell(row, 1).value == title:\n",
    "            row += 1\n",
    "        continue\n",
    "\n",
    "    # Grab the reviews for this title\n",
    "    reviews = []\n",
    "    while row <= max_row and sheet.cell(row, 1).value == title:\n",
    "        rev = sheet.cell(row, 6).value\n",
    "        if rev and rev.strip():\n",
    "            reviews.append(rev.strip())\n",
    "        row += 1\n",
    "    print(f\"Processing {title!r} with {len(reviews)} reviews.\")\n",
    "\n",
    "    final_question = get_main_question_dynamic(reviews)\n",
    "    response = ollama.chat(\n",
    "        model='qwen3:14b',\n",
    "        messages=[\n",
    "            {\"role\": \"user\",      \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": answer},\n",
    "            {\"role\": \"user\",      \"content\": final_question}\n",
    "        ]\n",
    "    )['message']['content']\n",
    "\n",
    "    # Strip off any '<‚Äã/think>' section if present\n",
    "    if '</think>' in response:\n",
    "        response = response.split('</think>', 1)[1]\n",
    "   \n",
    "    # Split into sentences by newline (and trim)\n",
    "    sentences = [line.strip() for line in response.split(\"\\n\") if line.strip()]\n",
    "    print(sentences)\n",
    "    \n",
    "\t# If sentences are bullet points, remove the bullet point\n",
    "    sentences = [s[2:] if s.startswith(\"- \") else s for s in sentences]\n",
    "\n",
    "    # Store under the sheet title\n",
    "    results[title] = sentences\n",
    "    print(f\"Collected {len(sentences)} concerns for {title!r}.\")\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open(OUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved {len(results)} papers' concerns to {OUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a070593",
   "metadata": {},
   "source": [
    "And here's the code of points 4 and 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c7f2743a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/fabbi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/fabbi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/fabbi/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rigorous Experimentation: Many submissions fall short on experimental rigor‚Äîinsufficient controls, lack of repeatability, vague protocol descriptions, or no statistical power analysis.', 'Comparison to Prior Work: Authors often claim novelty without fair, head-to-head benchmarks against the most relevant baselines.', \"Quantitative Claims Need Proof: Percentages are meaningless without a clear definition of the metric, baseline, and statistical significance. For example, 'Our method is 20% better' is vague without context.\", \"Dataset Size & Diversity: Too-small or homogenous datasets lead to overfitting and findings that don't generalize.\", 'Lack of comparison with relevant baselines: Authors did not compare with key existing methods (e.g., [1], [2], [3]) or failed to cite important related works on early-stopping and label noise.', 'Limited novelty: The proposed method is described as incremental or too similar to prior work (e.g., self-training, co-training, iterative learning with noisy labels).', 'Insufficient experimental validation: Experiments were conducted on small-scale datasets (e.g., CIFAR-10, CIFAR-100) without testing on larger, more challenging datasets (e.g., ImageNet, Clothing1M).', 'Missing theoretical analysis: Existing literature provides theoretical justification for early-stopping in label noise, but the paper lacks such analysis.', \"Inadequate discussion of results: The paper did not thoroughly analyze the method's effectiveness (e.g., maximal safe set illustration, comparison with more baselines, use of clean validation data).\", 'Lack of demonstration that PE score improvements lead to practical benefits in tasks like image generation or super-resolution.', 'Inadequate comparison to baselines (e.g., random feature selection, orientation-only/frequency-only metrics).', 'Insufficient experimental rigor, including small or non-diverse datasets, limited repeatability, and lack of statistical power analysis.', 'Ambiguity in defining and interpreting metrics (e.g., PE score calculation, use of derivatives/maxima instead of correlation).', 'Poor presentation of results (e.g., unclear figures, tables, and lack of visual clarity).', 'Limited connection to human perceptual studies or biological plausibility (e.g., comparison to human visual cortex data).', 'Overreliance on human perceptual data in PE score definition, potentially biasing results.', 'Lack of exploration of PE score limitations (e.g., hand-crafted features with maximal PE).', 'Unclear explanation of key concepts (e.g., CSF, contrast masking, DMOS, SROCC).', 'Incomplete or anecdotal experiments with limited generalizability.', 'Experimental results are limited to low-dimensional problems and lack comparison with state-of-the-art methods like CMA-ES.', \"Insufficient demonstration of the method's ability to achieve the stated goal of improving exploration and handling multi-modal distributions.\", 'Unclear or ambiguous technical details, including algorithmic steps, the use of NICE models, and the concept of \"global volume.\"', \"Limited validation on higher-dimensional benchmarks and challenging tasks that highlight the method's advantages over traditional ES approaches.\", \"Lack of clarity on how the proposed method's flexibility in the search distribution is effectively utilized during optimization.\", '**Insufficient novelty and significance** compared to existing works, with missed citations and lack of clear differentiation from prior research.', '**Low technical contribution** and limited theoretical advancements, with key results (e.g., Theorem 2, 3, 4) containing fundamental errors or being trivial extensions of known results.', '**Incomplete or unclear generalization bounds**, including failure to address conditions under which test error can be small.', '**Inadequate comparison with prior literature** on neural tangent kernels, interpolating kernel methods, and generalization bounds for overparameterized networks.', '**Errors in proofs and theorem statements**, such as incorrect bounds, misuse of uniformity in proofs, and inconsistent treatment of test data.', '**Lack of sufficient experimental validation** or discussion on the practical implications of theoretical findings.', '**Missing context on initialization scale** and its impact on the NTK regime, including potential oversight in relating initialization to network width.', \"Limited Technical Novelty: The paper's contributions are perceived as incremental or scattered, with minimal extensions beyond existing techniques like SASRec and SSE.\", 'Insufficient Experimental Validation: Results lack convincing evidence, including discrepancies in comparisons with prior work, unclear evaluation metric changes, and limited ablation studies.', 'Inadequate Baseline Comparisons: The experiments do not sufficiently compare with key baseline methods (e.g., BERT4Rec, Fossil) or address the effectiveness of the proposed approach rigorously.', 'Weak Justification for Design Choices: Questions remain about the necessity and effectiveness of specific components (e.g., sampling strategies, regularization techniques) without thorough analysis.', 'Insufficient comparison to strong baselines and lack of rigorous evaluation in zero-shot scenarios.', \"Weak theoretical/analytical justification for the method's effectiveness and assumptions.\", 'Limited ablation studies on key components of the proposed framework.', 'Concerns about the generalizability and feasibility of the approach across diverse tasks and environments.', 'Ambiguity in the experimental setup and reward signal handling for zero-shot generalization.', 'Lack of clarity in mathematical formulation and description of the method.', 'Questions about the necessity and impact of the proposed architecture for zero-shot RL.', 'Insufficient Experimental Validation: All reviews highlight the lack of comprehensive experiments, with qualitative results and limited dataset evaluation.', 'Limited Comparison with Existing Methods: No thorough comparison to traditional techniques (e.g., background subtraction) or established baselines (e.g., Glance, C-WSL).', 'Lack of Theoretical Justification: Techniques (e.g., Gini impurity, VAE connection) are criticized for being ad-hoc or lacking novelty and theoretical grounding.', 'Inadequate Evaluation Protocols: Experiments are not conducted on standard datasets (e.g., USCD, Trancos) with proper metrics (e.g., MAE, GAME).', 'Missing Ablation Studies: No analysis of individual components (e.g., pooling layers removal, rotation invariance) or their impact on performance.', 'Ambiguity in Weakly Supervised Framework: Use of point supervision in equations contradicts the weakly supervised claim.', '**Typos and Notation Errors**: Multiple serious typos, inconsistent notation, and missing definitions (e.g., \"mu\" in Fact 1, Lemma 9 in Lemma 1, missing constants in equations).', '**Theoretical Proof Issues**: Flawed or unclear derivations (e.g., equation (3) correction, missing constants in appendix, unclear assumptions in Big-O notation, and lack of justification for strong theoretical assumptions).', '**Experimental Concerns**: Irrelevant datasets in experiments (e.g., datasets with r < 10), lack of statistical significance, unclear metrics (e.g., \"relative deviation\" normalization without explanation), and missing standard deviations in results.', '**Conjecture and Explanation Gaps**: Hand-wavy conjectures on concentration of measure, unclear connection to real-world implications, and insufficient explanation of phenomena (e.g., minima at lambda=0.5 in Figure 3).', '**Implementation and Reproducibility**: No open code provided, lack of handling for sparsity in datasets, and unclear procedures for determining decomposition rank in practice.', '**Proof Clarity and Completeness**: Missing lemmas (e.g., Lemma 9), unclear proof steps (e.g., equation (9) derivation), and ambiguous definitions (e.g., \"measure\" in concentration of measure conjecture).', '**Optimization and Generalization**: Overlooking challenges in finding global minima despite convexity claims, and lack of discussion on generalization beyond the over-parameterized regime.', 'Lack of novelty: The method of adding noise for regularization is not novel and has been widely used in various applications.', \"Insufficient or unconvincing experiments: The experiments are small-scale, lack convincing results, and do not demonstrate the method's effectiveness on larger or more diverse datasets.\", 'Scalability concerns: The approach may not scale well to large datasets, as it resembles kernel density estimation, which is known to have scalability issues.', \"Asymptotic theoretical results: The theoretical analysis is asymptotic and does not provide practical insights into the method's advantages or performance.\", 'Unclear explanations: Key parts of the paper, such as the interpretation of terms in equations, are poorly explained or confusing.', 'Limited comparison with prior work: The paper does not adequately compare the method with existing techniques (e.g., Lipschitz regularization) or provide strong justification for its novelty.', \"Lack of empirical validation: The experiments do not sufficiently validate the method's benefits across different tasks or parametric families.\", 'Lack of novelty in the approach.', 'Limited experimental evaluation, with insufficient comparisons to prior work and lack of diverse datasets.', 'Poor writing quality, lack of clarity, and imprecise descriptions in the methodology and experiments.', \"Insufficient justification for the proposed method's uniqueness or motivation.\", 'Inadequate analysis of results, including missing ablation studies and theoretical insights.', 'Dataset limitations (e.g., too small, not generalizable, or not representative).', 'Poorly presented figures and algorithms, with issues like pixelation, lack of organization, and unclear explanations.', \"Lack of ablation studies to evaluate individual components' contributions.\", 'Reliance on existing methods (e.g., GMN) questions novelty.', 'Experiments on small graphs limit generalizability.', 'Insufficient comparison to alternative approaches (e.g., QAP-based baselines).', 'Use of heuristic stopping criteria without proper justification.', 'Potential improvements in graph embedding models (e.g., DGCNN over GCN).', 'Weak theoretical justification for post-processing normalization and GSE.', 'Limited analysis of scalability to larger graphs due to reliance on solver-supervised training.', 'Incremental technical contribution: The paper introduces only minor improvements (e.g., adding one feature and an attention layer) without substantial innovation.', 'Weak experimental results: Minimal performance gains (e.g., 0.2 F1 points) and lack of strong evidence to support the proposed method.', 'Limited dataset diversity: Experiments conducted on a single dataset (CoNLL 2012), restricting generalizability.', 'Unclear methodology: Lack of explanation for key components (e.g., grammatical numbers feature, benefits of attention mechanism).', 'Insufficient comparison to prior work: Misrepresentation of previous models (e.g., Lee et al. 2018) and lack of thorough evaluation.', 'Poor presentation: Issues with citation format, table clarity, and unclear equations/ablation results.', \"Minimal performance improvement: The marginal gains do not justify the paper's contribution for a top conference.\", '**Limited novelty** compared to prior work (e.g., Gidaris et al. 2019, multi-task learning with self-supervision is incremental).', '**Insufficient justification** for claims (e.g., self-supervised tasks outperforming data augmentation, effectiveness of domain selection).', '**Weak experimental setup** in Section 4.2 (e.g., unclear domain distance definition, flawed unlabeled pool methodology).', '**Trivial or predictable results** (e.g., performance gaps increasing with harder tasks, domain shift experiments not aligned with claims).', '**Over-reliance on image domain** and lack of generalization to other domains or tasks.', '**Inadequate comparison** to prior methods in similar settings (e.g., domain adaptation, semi-supervised learning).', '**Excessive scope** with many details relegated to the appendix, potentially reducing clarity.', '**Flawed domain shift analysis** (e.g., Figure 4d misrepresenting trends, lack of shared classes in extreme domain shift cases).', 'Overclaiming the reasoning aspect, with the model not actually performing reasoning but relying on manually defined constraints.', 'Lack of proper baselines or comparison to relevant prior work, particularly in tasks like Sudoku and generalization to other domains.', 'Poorly described experiments and unclear methodology, especially in the XRD section.', \"Use of pretrained generative decoders, which may limit the approach's applicability and novelty.\", 'Test set retraining (retraining on the test data) undermining the validity of results.', 'Unclear or unexplained dynamic weight updates for constraints.', \"Insufficient justification for the framework's generalizability beyond the specific tasks addressed.\", 'Weak connection between the proposed work and the concept of \"reasoning\" as advertised.', 'Lack of novelty or significant contribution', 'Insufficient experimental validation and lack of comparison to prior work', 'Inadequate analysis and lack of theoretical justification', 'Limited dataset diversity and scale', 'Practical benefits not demonstrated', 'Lack of significant differentiation from prior work (e.g., [Xiangru Lian et al., 2017], [A.Devraj & J.Chen (2019)], [1][2]).', 'Limited experimental scope and diversity (e.g., only nonlinear-embedding problems tested).', 'Convergence results not clearly superior to existing methods (e.g., [1][2]).', 'Missing key related work citations.', 'Insufficient empirical comparisons with state-of-the-art methods.', 'Lack of clear validation for the claim that the i.i.d. assumption is invalid.', 'Insufficient theoretical justification for the proposed probabilistic models (e.g., Gibbs distribution, PoE).', 'Weak comparison with existing methods (e.g., RBMs, BHMs) and unclear differentiation of the approach.', 'Ambiguity in the role and effectiveness of the proposed regularization term.', 'Concerns about the practical impact of the method on widely used networks (e.g., ResNet).', 'Poor clarity and writing quality, making the paper difficult to follow.', 'Insufficient explanation of the relationship between the proposed model and Bayesian inference.', 'Lack of rigorous experiments or analysis to support the claims.', 'Lack of clear motivation and explanation for the theoretical contribution of tropical geometry.', 'Insufficient comparison to prior work and missing relevant references.', 'Ambiguity in the definition and interpretation of key concepts (e.g., tropical quotient, upper faces, normals).', 'Unclear or overly technical explanations of theorems and their implications.', 'Concerns about the experimental focus being too broad and lacking depth.', 'Inadequate clarification of the adversarial attack approach and its relevance.', 'Issues with figure descriptions and visualizations (e.g., Figure 2, Figure 4).', 'Need for more detailed experimental results, including repetitions and standard deviations.', 'Questions about the uniqueness and practical implications of the functions described in Theorem 2.', 'Suggestions to improve clarity for non-experts and build intuition for complex concepts.', 'Ambiguity in the relationship between the dual subdivision and the decision boundaries.', 'Lack of discussion on the implications of the adversarial attack method.', 'Terminology concerns (e.g., \"nuisance\" for adversarial attacks).', 'Need for better organization and focus in the experimental sections.', 'Lack of clear motivation and explanation for the ideas presented.', 'Poor writing style, unclear explanations, and grammatical errors.', 'Insufficient experimental validation with limited and simple test cases.', 'Need for more thorough and comprehensive experiments.', 'Theoretical contributions not well communicated or contextualized.', 'Limited novelty/originality, relying heavily on existing components.', 'Insufficient experimental analysis and ablation studies.', 'Experiments conducted in overly simple/synthetic environments (e.g., VizDoom) without testing in more realistic or complex settings.', 'Lack of theoretical justification or explanation for key design choices (e.g., projective geometry).', 'Inadequate comparison to prior work and failure to demonstrate distinct technical contributions.', 'Missing experiments on tasks where the inductive bias may not be necessary or beneficial.', \"Lack of Theoretical Guarantees: All reviews highlight the absence of rigorous theoretical analysis, such as regret bounds or convergence guarantees, which weakens the paper's contribution.\", 'Insufficient Experimental Comparison: The paper lacks comparisons with relevant baselines, especially methods addressing catastrophic forgetting (e.g., regularization techniques) and other state-of-the-art approaches.', 'Limited Dataset Diversity: Experiments are conducted on narrow datasets (e.g., supervised learning tasks, not RL datasets), raising concerns about generalizability and real-world applicability.', 'Missing Experimental Details: Critical information about experiment setups (e.g., reward definitions, SDP solver accuracy, hyperparameters) is omitted, reducing reproducibility and transparency.', \"Scalability and Generalizability Concerns: The method's reliance on linear features and its linear complexity in action number raises doubts about scalability to more complex or high-dimensional settings.\", 'Weak Justification for Key Components: The effectiveness of the proposed likelihood matching and covariance approximation lacks thorough analysis or explanation, particularly in nonlinear settings.', 'Limited Novelty: The method is based on existing approaches like slimmable networks and DropPath, with insufficient differentiation from prior work.', 'Inadequate Comparison to Baselines: Lack of fair comparison with pruning methods (e.g., AMC, ThiNet) and insufficient clarification on how the proposed method differs from existing techniques.', 'Missing Experimental Details: Insufficient ablation studies, unclear training protocols, and lack of information on starting models, hyperparameters, and search costs.', 'Inconsistent Baseline Evaluation: Baselines (e.g., MobileNet, ResNet) may not be trained under the same conditions, affecting the validity of comparisons.', 'Ambiguity in Methodology: Unclear justification for architectural choices (e.g., channel distribution in deep layers) and lack of details on transferability, training procedures, and time complexity.', 'Missing Captions/Data: Figures and tables lack captions, and some critical metrics (params, memory, latency) are omitted.', 'Lack of clear explanation for implementation details (e.g., masking strategy, grid search methodology).', 'Insufficient comparison to prior work and unclear justification for non-trivial extensions.', 'Overreliance on heavy data augmentation, limiting valid comparisons to other methods.', 'Lack of theoretical insights or generalization to other tasks/datasets (e.g., CIFAR, MNIST).', 'Need for ablation studies and more comprehensive experiments (e.g., super-net performance, search efficiency).', 'Missing figures/clarifications (e.g., search space visualization, statistical reporting in figures).', 'Ambiguity in methodological motivations and combination of existing techniques.', 'Lack of proper comparison to prior work, particularly Song et al. (2018) and others.', 'Use of different norms (e.g., L2 vs. L-infinity) leading to unfair comparisons in experiments.', \"Concerns about the method's effectiveness in preserving semantics, especially in NLP tasks.\", 'Weak pilot study with limited sample size and unclear evaluation questions in NLP examples.', 'Terminology inconsistency (using \"adversarial success rate\" instead of \"attack success rate\").', 'Inadequate alignment with existing adversarial attack frameworks, particularly for black-box vs. white-box scenarios.', 'Arbitrary choices in the algorithm with unclear motivation and lack of theoretical guarantees.', 'Misuse of reparameterization trick and variational inference objective (missing KL-divergence term).', 'Lack of proper comparison with state-of-the-art OOD detection methods (e.g., Mahalanobis distance, ODIN).', 'Insufficient theoretical motivation for assuming unimodal Gaussian distributions across latent spaces.', 'Empirical results underperforming compared to prior work (e.g., lower classification accuracy on benchmarks).', 'Unclear explanation of how features are used for OOD detection and the role of mu/sigma in the model.', 'Insufficient ablation studies to validate the effectiveness of sigma over mu or other components.', 'Concerns about model convergence and degenerate solutions due to stochasticity in training.', 'Poor clarity in explanations, ambiguous terminology, and lack of detailed methodology descriptions.', 'Inadequate justification for splitting mu and sigma, and discarding mu for OOD detection.', 'Small number of training epochs and unclear training procedures impacting experimental validity.', '**Clarity and Explanation of Equations/Methodology**: Vague or insufficient explanations of key equations (e.g., Equation 2), unclear definitions of variables (e.g., z), and lack of context before introducing complex components (e.g., hierarchical LSTM).', '**Comparison to Baselines/State-of-the-Art**: Lack of fair benchmarks against attentional methods or standard models (e.g., IMDb, MNIST).', '**Dependence on Hand-Crafted Cognitive Chunks**: Reliance on task-specific chunking strategies without clear criteria or exploration of alternatives.', '**Weak Experimental Section**: Insufficient analysis of how parameters (e.g., bottleneck thickness *k*) affect results, unclear evaluation metrics, and ambiguous interpretations of human judgments.', '**Insufficient Discussion of Approximator Quality**: Lack of analysis on how the approximator‚Äôs fidelity (e.g., *q(y|t)*) impacts interpretability and alignment with *p(y|x)*.', '**Ambiguities in Methodology Descriptions**: Unclear or overly technical phrasing (e.g., \"Negative Sentiment if any negative words\"), lack of citations for novel terms (e.g., hierarchical LSTM), and undefined symbols/notations.', '**Missing Inter-annotator Agreement or Reproducibility**: No mention of human evaluation reliability or standard splits in datasets (e.g., IMDb).', '**Theoretical vs. Practical Interpretability**: Concerns that theoretical compression (e.g., minimal sufficient statistic) may not align with human interpretability or comprehensiveness.', '**Inadequate Addressing of Limitations**: Lack of discussion on methodological shortcomings (e.g., reliance on approximators, chunking strategies) or potential biases in experiments.', 'Experiments lack comparison with recent state-of-the-art methods (e.g., symmetric cross entropy from ICCV2019, Lee et al. 2019 ICML).', 'Insufficient testing on asymmetric noise and non-vision tasks.', 'Lack of theoretical justification for claims about emphasis focus/spread and their impact on robustness.', 'Hyperparameters (alpha, beta) require extensive tuning with no clear guidance, risking overfitting.', 'Limited experimental validation (e.g., no confidence intervals, inconsistent baselines across datasets).', 'Ambiguous definitions and unclear mathematical formulations (e.g., emphasis focus/spread, \"semantically abnormal examples\").', 'Interaction with optimizers (e.g., Adam) and scalability on small datasets not addressed.', 'Insufficient clarity in methodology and experimental setup for fair comparisons.', 'Theoretical analysis lacks clarity and contains unjustified assumptions, making it unclear how the proposed method relates to the claimed variance reduction.', 'Experimental improvements are not statistically significant or well-justified, with limited comparison to existing methods across datasets.', 'The conceptual contribution is incremental and not sufficiently distinct from existing normalization techniques like batch normalization.', \"The paper's writing and explanations are unclear, particularly for non-experts, with ambiguous notation and insufficient intuitive explanations.\", 'The relationship between the proposed method and the theoretical analysis is not fully aligned, including discrepancies in architecture and assumptions.', 'The variance reduction mechanism and key measures (e.g., *K*) are not clearly defined or validated experimentally.', 'The use of certain parameters (e.g., sigma_g, sigma_s) and the design choices (e.g., two-layer neural network for weight parameterization) lack sufficient motivation or explanation.', 'Lack of Novelty: The proposed methods are seen as combinations of existing techniques without sufficient original contributions that justify publication in top conferences.', 'Insufficient Experimental Validation: Ablation studies and detailed comparisons to demonstrate the effectiveness of the claimed contributions (e.g., two-time-scale structure, progressive training) are missing or inadequate.', 'Unclear Technical Explanations: Key aspects of the methodology (e.g., loss function choice, progressive training strategy, equation formulations) are not sufficiently explained or justified.', 'Limited Theoretical/Analytical Justification: The framework lacks rigorous theoretical analysis or deeper insights into why the proposed methods work, relying heavily on empirical results.', 'Weak Contribution Claims: The novelty of the contributions (e.g., residual-based training, long temporal fusion) is questioned, with reviewers suggesting they are trivial or not sufficiently distinct from prior work.', 'Poor technical writing and clarity of presentation.', 'Insufficient experimental evaluation, including limited comparison with state-of-the-art methods.', 'Lack of robustness analysis and stability concerns in the proposed method.', 'Limited evaluation on diverse or real-world domains.', \"Inadequate theoretical justification or analysis of the method's effectiveness.\", '**Theoretical guarantees** are questioned for their assumptions, clarity, and practical relevance.', '**Hyperparameter tuning** (e.g., Delta, burn-in epochs) lacks detailed justification and validation.', '**Computational cost** comparison between proposed and standard methods is unclear.', '**Generalization to multiclass settings** requires more explanation.', '**Real-world dataset experiments** (e.g., Clothing1M) are missing or underemphasized.', '**Time complexity** and **convergence analysis** are not addressed.', '**Comparison to noise-free data** and detailed baselines are requested.', '**Theoretical bounds** are criticized for being vacuous or overly dependent on opaque constants.', '**Assumptions in theorems** (e.g., linear model relationship) are deemed unrealistic or unproven.', '**Empirical robustness** to hyperparameters needs clearer validation.', 'Lack of clear motivation and comparison to prior work (e.g., combination of data augmentation with explicit regularization).', 'Insufficient experimental rigor (e.g., ImageNet resolution mismatch, suboptimal results compared to state-of-the-art).', 'Inadequate hyperparameter tuning for explicit regularization and data augmentation.', 'Vague definitions of explicit vs. implicit regularization.', 'Limited theoretical analysis (e.g., reliance on Rademacher complexity without deeper insights).', 'Narrow empirical scope (focus on image classification, lack of experiments in other domains like NLP).', 'Missing comprehensive survey of regularization techniques across tasks and domains.', 'Incomplete or unclear description of data augmentation parameters and protocols.', 'Lack of head-to-head benchmarks against relevant baselines.', 'Insufficient discussion of the non-i.i.d. nature of augmented data in theory.', 'Ambiguity in the paper‚Äôs claims about the effectiveness of data augmentation over explicit regularization.', '**Reward bias and its handling**: Concerns about whether SAIL effectively addresses reward bias and whether the non-negative reward design is ad-hoc or insufficient for general cases.', '**Experimental setup and results consistency**: Questions about state distribution mismatch in Lunar Lander, inconsistency between figures (e.g., Figure 4 vs. Table 2), and lack of statistical significance or detailed metrics in results.', '**Comparison to prior work**: Lack of fair benchmarks against relevant baselines (e.g., DAC, AIRL, IRL methods) and unclear justification for novelty.', '**Theoretical guarantees**: Absence of optimality guarantees for the learned policy and unclear theoretical analysis (e.g., assumptions about GAIL‚Äôs support estimation).', '**Support estimation and reward reliability**: Concerns about the reliability of RED‚Äôs reward and whether the combined SAIL reward truly improves stability and generalization.', '**Implementation and naming clarity**: Ambiguity in experimental naming (e.g., GAIL vs. GAIL-log) and lack of clarity in algorithmic descriptions.', '**Stochasticity and evaluation**: Questions about the impact of stochastic policies on demonstration and evaluation trajectories, and missing data on demonstration reward distributions.', '**Statistical analysis and variance**: Insufficient reporting of standard deviations, variance in results (e.g., Half-Cheetah), and lack of statistical power analysis.', '**Generalization and robustness**: Doubts about the method‚Äôs generalization to MDPs where Q-values outside the expert support may dominate, and lack of analysis for non-deterministic expert policies.', 'Insufficient clarity in explaining the *Bernoulli splitting trick*.', 'Theoretical analysis lacks quantification of estimator variance, especially after modifications.', 'Experiments are incomplete, with missing ablations and comparisons to key baselines (e.g., Gumbel-Softmax, REBAR).', 'Weak connection between theoretical contributions and empirical results.', 'Poor readability, formatting issues, and typos in the paper.', 'Theoretical derivations (e.g., Fourier analysis) are criticized as unnecessary or overly complex.', 'Concerns about the correctness of proofs and assumptions (e.g., Lemma 3‚Äôs proof).', 'Missing practical guidance for hyperparameters (e.g., representation scaling).', '**Novelty concerns**: The proposed method may not be sufficiently novel, with claims of innovation overlapping with existing work (e.g., RP trees, X-Projection trees, beta-similarity).', '**Insufficient experimental rigor**: Lack of statistical analysis, repeated experiments, or clarification of results (e.g., single-run experiments, unclear significance of results in Fig. 6).', '**Inadequate comparison to prior work**: Missing comparisons to relevant baselines (e.g., multi-partition-based methods, Mahalanobis distance, Extremely randomized trees).', '**Ambiguous definitions and claims**: Unclear explanations of key terms (e.g., DIS_i(X, Y), m, beta parameter), and vague justifications for the method‚Äôs perceptual or prior knowledge independence.', '**Conceptual clarity issues**: The X-Projection tree and beta-similarity lack clear conceptual differentiation from existing methods (e.g., layer-by-layer RP trees, random forests).', '**Overstated or unproven claims**: Assertions about perceptual similarity, independence from prior knowledge, or superiority over Euclidean distance lack support.', '**Implementation and reproducibility**: Missing details on implementation (e.g., code, hyperparameters), and unclear whether speed gains are due to algorithmic improvements or implementation choices.', '**Language and presentation flaws**: Poor writing, grammatical errors, and unclear structure (e.g., vague introduction, confusing definitions).', '**Evaluation limitations**: Reliance on label-based clustering metrics without considering unsupervised alternatives, and lack of exploration of beta-similarity parameter sensitivity.', 'Lack of thorough evaluation and insufficient comparison with prior work (e.g., Harb 2018, Hausman 2018, OptionCritic).', 'Unclear definitions and justification for \"near-optimal\" policies and their metrics.', 'Weak demonstration of multi-task reusability and generalization across tasks.', 'Insufficient explanation of how options are learned and validated, including unclear source of \"near-optimal\" trajectories.', 'Limited experimental scope (e.g., few Atari games, lack of high-dimensional or continuous control benchmarks).', 'Poor clarity in the paper‚Äôs writing, including ambiguous descriptions of tasks, objectives, and evaluation methods.', 'Lack of Technical Contribution: The paper is criticized for not presenting novel technical methods, focusing instead on data collection and description without clear innovation.', 'Insufficient Experimental Demonstration: No experiments are provided to show how the knowledge graph (KG) benefits machine learning tasks or improves upon existing benchmarks.', 'Misalignment with Conference Scope: The work is deemed unsuitable for ICLR due to its focus on data curation rather than learning representations or deep learning.', 'Weak Related Work Section: The paper fails to clearly address limitations of existing KGs or explain how the proposed tech-specific KG overcomes them.', 'Lack of Comparison to Prior Work: No benchmarking against existing techniques for tasks like name deduplication or KG construction.', 'Ambiguous Terminology: The paper is criticized for mislabeling a bibliographic database as a KG, without sufficient justification for its semantic richness.', 'Missing Key Features: The absence of a citation graph and unclear methodologies for defining hierarchical relations or data sources.', 'Poor Integration of Appendix: The appendix contains valuable information but is not effectively distilled into the main paper.', 'Lack of algorithmic novelty: The paper uses conventional machine learning methods (e.g., K-NN, XGBoost, SVM) without introducing novel algorithms or theoretical contributions.', \"Poor alignment with ICLR's focus: The work is deemed too applied, industry-specific, or lacking in relevance to the core research interests of the ICLR community.\", \"Insufficient experimental validation: The experiments are criticized for not adequately demonstrating the method's advantages or providing rigorous comparisons to baselines.\", 'Issues with writing and clarity: The paper is described as poorly written, redundant, or overly focused on low-level implementation details rather than high-level contributions.', '**Theoretical validity and generalization**: Concerns about the theory\\'s applicability to non-convex NAS, potential \"gradient traps,\" and whether the proposed method fully addresses the problem.', \"**Empirical rigor**: Lack of ablation studies, insufficient analysis of where improvements come from, and unclear evidence for the proposed solution's effectiveness.\", '**Comparison to baselines**: Inadequate or unfair comparisons with existing methods (e.g., DARTS, SOTA techniques like early stopping), and limited generalization to other approaches.', '**Tractability of Hessian-based computations**: Questions about the feasibility of computing the Hessian matrix, especially in high-dimensional spaces, and how the proposed approximation avoids intractability.', \"**Clarity of experiments**: Need for more detailed experiments (e.g., learning curves, toy examples of gradient traps) and clearer demonstrations of the method's impact.\", '**Writing and exposition**: Redundant equations, unclear terminology (e.g., \"auxiliary loss tower\"), and grammatical/structural issues in the paper.', \"**Contribution and novelty**: Perceived limitations in the method's novelty (similar techniques exist in hyperparameter optimization) and its contribution compared to existing SOTA methods.\", '**Computational efficiency**: Questions about the reported speedup in computation and the choice of search space for experiments.', 'Theoretical analysis is insufficient or lacks depth.', 'Need for ablation studies on hyperparameters and edge feature impact.', 'Comparison to prior work, especially infomax.', 'Motivation for focusing on edge features rather than node features.', 'Experimental results interpretation (e.g., training loss improvement).', 'Citing foundational work (infomax by Linsker).', 'Potential for trivial solutions in the loss formulation.', 'Lack of exploration on the effect of hyperparameters on model performance.', 'Lack of reproducibility details (hyperparameters, tuning justification, repeated runs).', 'Insufficient clarity in methodology and terminology (e.g., unclear acronyms, ambiguous claims, lack of theoretical support).', 'Weak positioning relative to existing literature (e.g., overlooked relevant works, unclear motivation).', 'Grammatical and writing issues affecting readability and professionalism.', 'Unclear or inconsistent explanations of key components (e.g., message categorization, POSs in Algorithm 1).', 'Limited discussion of scalability trade-offs and practical implications.', 'Limited experimental scope and lack of comparison with state-of-the-art methods on diverse datasets (e.g., image recognition, NLP, Office-Home, DomainNet).', 'Insufficient evaluation on benchmark domain adaptation datasets (e.g., Office, OfficeHome) to demonstrate generalizability.', 'Weak novelty claim due to similarity to existing methods (e.g., contrastive loss, Siamese GANs, Energy-based GANs).', 'Inadequate explanation of methodological differences from related works (e.g., MDAT vs. SiGAN, DIRT-T, Maximum Classifier Discrepancy).', 'Poorly addressed hyperparameter selection and unclear notation (e.g., asterisk in Table 1, typos in equations).', 'Lack of interpretability analysis and stability evaluation on large-scale or complex datasets.', 'Missing critical references to relevant prior work (e.g., Zhao et al., 2016).', 'Limited Novelty and Incremental Contributions: The proposed architecture is seen as a minor variation of standard MLPs, with insufficient justification for its distinctiveness.', 'Weak Empirical Results: Improvements over baselines are marginal (often <1%), with limited demonstration of effectiveness on diverse or large-scale datasets.', 'Insufficient Theoretical/Intuitive Explanation: Lack of clear motivation for key design choices (e.g., grouping mechanism, softmax normalization, parameter relationships).', 'Over-Reliance on Small/Homogeneous Datasets: Experiments are conducted on small or limited datasets (e.g., MNIST, CIFAR-10), raising concerns about generalizability.', 'Poor Visualization/Analysis of Group-Select Mechanism: Limited or unclear insights into how groups are formed or their impact on performance.', 'Missing Ablation Studies and Hyperparameter Analysis: Critical components (e.g., group selection, annealing schedules, window sizes) lack thorough investigation.', 'Inadequate Comparison to Prior Work: Insufficient benchmarking against relevant baselines or failure to address limitations of existing methods.', 'Ambiguity in Methodology: Unclear details on implementation (e.g., pixel permutation, group selection criteria, parameter settings) hinder reproducibility.', 'Inconsistent or unsupported experimental results.', 'Confusing or contradictory claims in the paper.', 'Lack of clarity in methodology and presentation.', 'Insufficient discussion on combining techniques or determining the right approach.', 'Minor writing and formatting issues affecting readability.', '**Misapplication of Reinforcement Learning (RL):** The method is not properly grounded in RL principles, with unclear state transitions and a lack of decision-making at each time step, suggesting it may be better classified as a Multi-Armed Bandit (MAB) problem.', '**Necessity of RL:** The use of RL is questioned as non-essential, with suggestions that differentiable approximations or simpler methods (e.g., random sampling, straight-through estimators) could achieve similar results more efficiently.', '**Lack of Baseline Comparisons:** No rigorous comparisons to differentiable alternatives or simpler baselines are provided to validate the necessity and effectiveness of the RL-based approach.', '**Insufficient Analysis of Sample Efficiency:** The paper does not address or compare the sample efficiency of the RL method, particularly in relation to potential differentiable alternatives.', '**Limited Insight into Weighting Function Behavior:** The weighting function‚Äôs behavior (e.g., entropy, sample selection patterns) is not analyzed, leaving unclear how or why specific samples are chosen.', '**Ambiguity in Methodology Justification:** The paper lacks clear justification for the non-differentiable subsampling approach and its overhead, particularly in dynamic settings where the dataset may change.', 'Lack of comparison to relevant baselines', 'Limited ablation studies to evaluate design choices', 'Dataset limitations (small, not diverse tasks)', 'Hyperparameters and reproducibility issues (e.g., fixed values, unclear architecture)', 'Choice of discrete representation for the data value estimator (motivation unclear)', '**Lack of clear comparison to prior work** (e.g., steerable nets, gauge equivariant nets, capsule networks).', '**Insufficient experimental validation** (limited datasets, unclear methodology, no baseline comparisons).', '**Ambiguity in theoretical/practical connections** (e.g., how group theory maps to implementation, parameter sharing, computational overhead).', '**Confusing or vague experimental setup** (e.g., data augmentation vs. proposed method, fine-tuning procedure, parameter count claims).', '**Overlooked or underemphasized limitations** (e.g., finite group constraints, lack of generalization to continuous symmetries).', '**Inadequate demonstration of novelty** (similar to existing works, no clear differentiation from prior methods).', '**Poorly explained key concepts** (e.g., equivariance, orbits, variable sharing, group actions).', '**Insufficient empirical results** (e.g., no analysis of rotation robustness, lack of performance metrics, unclear figures).', '**Typos, grammatical errors, and unclear writing** (impeding readability and understanding).', '**Unaddressed scalability issues** (parameter explosion, computational complexity, and network size claims).', 'Theoretical analysis is unclear or lacks sufficient explanation.', \"Insufficient experimental validation of the model's contributions, particularly regarding the impact of LPA.\", 'Lack of connection between theoretical analysis and practical features described in experiments.', 'Marginal improvements in results and limited comparison with varying labeled samples.', 'Need for better explanations or graphical aids in proofs.', 'Insufficient emphasis on novelty and lack of clear articulation of contribution.', 'Overlap with prior work (e.g., Fitted Q-iteration by Advantage Weighted Regression), leading to incremental contributions.', 'Weak experimental results: failure to outperform existing methods on standard benchmarks (e.g., MuJoCo tasks).', 'Inadequate benchmark selection (e.g., using uncommon tasks like LunarLander instead of standard ones).', 'Insufficient experimental rigor (e.g., using only 5 seeds for MuJoCo experiments).', 'Lack of theoretical or empirical analysis of key components (e.g., value function estimation stability).', 'Missing citations to closely related work, weakening the novelty claim.', 'Ambiguities in equations, algorithm descriptions, and code implementation details.', 'Lack of Novelty: The proposed methods are criticized as simple combinations of existing techniques without addressing fundamental challenges in style transfer.', 'Insufficient Experimental Analysis: Limited exploration of hyperparameters, lack of systematic studies on the effect of combining multiple methods, and minimal quantitative evaluation.', 'Missing Literature Review: The paper fails to thoroughly compare with prior work on mixing styles or address key references in the field.', 'Reliance on Qualitative Results: Overemphasis on qualitative comparisons without robust quantitative analysis to validate claims of improvement.', 'Incomplete/Unconvincing User Studies: Poorly designed or inadequately reported human preference studies, with missing details on methodology, statistical significance, and bias control.', 'Lack of clear differentiation from prior work (e.g., similarity to CenterNet, FCOS, DeepMask).', 'Insufficient comparison with existing methods in terms of performance, speed, and hyperparameters.', 'Limited experimental analysis (e.g., ablation studies on other datasets, computational cost).', 'Discretization of scale still resembling anchor-based approaches.', 'Ambiguity in addressing overlapping bounding boxes and inference efficiency.', 'Writing clarity and technical explanation issues (e.g., unclear figures, confusing descriptions).', 'Lack of theoretical analysis and depth in experiments', 'Small or unclear empirical improvements in accuracy', 'Need for reporting variance across multiple experimental runs', 'Insufficient comparison to prior work (e.g., DARTS, [4], Xie et al.)', 'Terminology issues (e.g., \"searching space\" vs. \"search space,\" \"topology\" vs. \"soft topology\")', 'Ambiguity in methodology and setup (e.g., Table 3, sparsity regularization)', 'Lack of clarity in explanations and figures', 'Limited novelty compared to existing methods (e.g., DARTS, MaskConnect)', 'Missing related work in neural architecture search (NAS) context', 'Concerns about the definition and discretization of \"topology\"', 'Inadequate explanation of baselines and their differences from prior approaches', \"Theoretical Justification: Lack of sufficient theoretical analysis or guarantees (e.g., certified robustness) to support the method's effectiveness.\", 'Comparison to State-of-the-Art: Insufficient comparison with existing methods (e.g., TRADES, Carlini‚Äôs work) or evaluation under different threat models (e.g., L2, unrestricted attacks).', 'Evaluation Rigor: Need for more thorough evaluation, including adaptive attack testing and validation of claims (e.g., the \"limiting cycle\" argument in Figure 1).', 'Training and Generalization: Limited discussion on training challenges, heuristics, or how the framework generalizes beyond the tested scenarios (e.g., CIFAR-10/100).', 'Practical Utility: Concerns about the framework‚Äôs practicality, including the effectiveness of the attacker network and its impact on defense performance.', \"Lack of Clarity in Algorithm Description: The paper fails to clearly summarize the complete algorithm, including input, output, and parameters, leading to confusion about the method's structure.\", 'Insufficient Comparison to Prior Work: The paper does not adequately compare with relevant baselines (e.g., community-preserving node embedding methods or existing spectral approaches), reducing the perceived novelty.', 'Inconsistent or Incomplete Empirical Evaluation: Results are compared using different measures across tables, and experiments lack scalability/efficiency analysis on large graphs or comparisons with alternative two-step methods.', 'Limited Novelty: The approach is seen as combining existing techniques without sufficient differentiation from prior work (e.g., methods that transfer matrix E to H to P).', \"Ambiguity in Theoretical Justification: The paper lacks detailed explanation of the learning process (e.g., how the node embedding matrix E is obtained) and deeper analysis of the method's theoretical underpinnings.\", 'Inadequate Analysis of Results: Trends in experimental results (e.g., increasing/decreasing performance with batch size) are not thoroughly explained, and the correlation between detected communities and original labels is not investigated.', 'Lack of comparison to established baselines (e.g., BERTSUM, DUC datasets, GPT-2).', 'Insufficient evaluation on diverse or human-annotated datasets (e.g., DUC, real-world summarization tasks).', 'Missing ablation studies to analyze the contribution of key components (e.g., lead bias, filtering criteria).', 'No human evaluation to validate the quality of generated summaries.', 'Inconsistent or unclear experimental setup (e.g., varying hyperparameters, metrics, data filtering criteria).', 'Lack of statistical significance analysis for reported results.', 'Concerns about novelty, with claims of non-unique ideas (e.g., lead bias in datasets).', 'Ambiguous or incomplete explanations in the paper (e.g., unclear methodology, unclear sentences in sections).', 'Lack of detailed model architecture and training process descriptions', 'Insufficient experimental details (e.g., hyperparameter tuning, evaluation protocols, statistical analysis)', 'Limited dataset diversity and lack of comparison to standard benchmarks', 'Ambiguous or missing theoretical/motivational justification for method choices', 'Poor presentation clarity (e.g., typos, unclear figures, inconsistent equations)', 'Inadequate explanation of key components (e.g., loss function behavior, source generation)', 'Missing ablation studies or analysis of critical parameters (e.g., annealing schedules, window sizes)', 'Overreliance on qualitative results without quantitative validation', 'Lack of discussion on generalizability across tasks or domains', 'Incomplete or vague descriptions of algorithms and their potential edge cases']\n",
      "- **Experimental Concerns**: Irrelevant datasets in experiments (e.g., datasets with r < 10), lack of statistical significance, unclear metrics (e.g., \"relative deviation\" normalization without explanation), and missing standard deviations in results.\n",
      "- Inadequate Analysis of Results: Trends in experimental results (e.g., increasing/decreasing performance with batch size) are not thoroughly explained, and the correlation between detected communities and original labels is not investigated.\n",
      "- **Misapplication of Reinforcement Learning (RL):** The method is not properly grounded in RL principles, with unclear state transitions and a lack of decision-making at each time step, suggesting it may be better classified as a Multi-Armed Bandit (MAB) problem.\n",
      "- **Low technical contribution** and limited theoretical advancements, with key results (e.g., Theorem 2, 3, 4) containing fundamental errors or being trivial extensions of known results.\n",
      "- **Clarity and Explanation of Equations/Methodology**: Vague or insufficient explanations of key equations (e.g., Equation 2), unclear definitions of variables (e.g., z), and lack of context before introducing complex components (e.g., hierarchical LSTM).\n",
      "- **Theoretical Proof Issues**: Flawed or unclear derivations (e.g., equation (3) correction, missing constants in appendix, unclear assumptions in Big-O notation, and lack of justification for strong theoretical assumptions).\n",
      "- Insufficient Experimental Validation: Ablation studies and detailed comparisons to demonstrate the effectiveness of the claimed contributions (e.g., two-time-scale structure, progressive training) are missing or inadequate.\n",
      "- **Implementation and reproducibility**: Missing details on implementation (e.g., code, hyperparameters), and unclear whether speed gains are due to algorithmic improvements or implementation choices.\n",
      "\n",
      "\n",
      "# üìù Paper Writing Guidelines for Authors\n",
      "\n",
      "This guide outlines key principles to help authors avoid common pitfalls and meet the expectations of peer reviewers and the broader research community.\n",
      "\n",
      "## 1. üß™ Experimental Rigor and Statistical Validity\n",
      "\n",
      "**Key Concerns:** Irrelevant datasets (e.g., datasets with r < 10), lack of statistical significance, ambiguous metrics (e.g., unexplained \"relative deviation\" normalization), and missing standard deviations in results.\n",
      "\n",
      "**Best-Practice Solutions:**\n",
      "\n",
      "1. **Use Relevant and Representative Datasets**\n",
      "   - Ensure datasets align with the problem‚Äôs scope and are sufficiently sized (e.g., avoid datasets with r < 10).\n",
      "   - Justify dataset selection in the methodology section.\n",
      "\n",
      "2. **Report Statistical Significance**\n",
      "   - Include p-values, confidence intervals, or hypothesis tests (e.g., t-tests, ANOVA) to validate results.\n",
      "   - Clearly define the significance threshold (e.g., Œ± = 0.05).\n",
      "\n",
      "3. **Define Metrics and Normalization**\n",
      "   - Explain all metrics (e.g., \"relative deviation\") in detail, including formulas and normalization logic.\n",
      "   - Avoid vague terms; use standard benchmarks where applicable.\n",
      "\n",
      "4. **Report Variability in Results**\n",
      "   - Always include standard deviations, error bars, or other measures of variability in tables and figures.\n",
      "   - Use consistent formatting for statistical reporting (e.g., mean ¬± std).\n",
      "\n",
      "---\n",
      "\n",
      "## 2. üîç Comprehensive Analysis of Results\n",
      "\n",
      "**Key Concerns:** Unexplained trends (e.g., performance changes with batch size), lack of analysis on correlations (e.g., communities vs. original labels), and insufficient discussion of limitations.\n",
      "\n",
      "**Best-Practice Solutions:**\n",
      "\n",
      "1. **Interpret Trends and Patterns**\n",
      "   - Provide detailed explanations for observed trends (e.g., why performance increases with batch size).\n",
      "   - Use visualizations (e.g., line graphs, heatmaps) to support claims.\n",
      "\n",
      "2. **Investigate Correlations and Relationships**\n",
      "   - Explicitly analyze relationships between variables (e.g., correlation between detected communities and original labels).\n",
      "   - Use statistical tools (e.g., Pearson‚Äôs r, chi-square tests) where appropriate.\n",
      "\n",
      "3. **Discuss Limitations and Trade-Offs**\n",
      "   - Acknowledge limitations of the approach (e.g., scalability issues, data biases).\n",
      "   - Compare trade-offs between different design choices (e.g., speed vs. accuracy).\n",
      "\n",
      "---\n",
      "\n",
      "## 3. üéØ Proper Application of Methodologies\n",
      "\n",
      "**Key Concerns:** Misapplication of reinforcement learning (RL) principles, unclear state transitions, and misclassification as multi-armed bandit (MAB) problems.\n",
      "\n",
      "**Best-Practice Solutions:**\n",
      "\n",
      "1. **Ground Methods in Theoretical Foundations**\n",
      "   - Clearly define the problem as RL if applicable, ensuring alignment with principles like state transitions and reward modeling.\n",
      "   - Avoid conflating RL with MAB unless explicitly justified.\n",
      "\n",
      "2. **Detail Decision-Making Processes**\n",
      "   - Explain how decisions are made at each time step (e.g., policy updates, action selection).\n",
      "   - Use diagrams or pseudocode to clarify complex interactions.\n",
      "\n",
      "3. **Differentiate from Simpler Frameworks**\n",
      "   - If the method resembles MAB, explicitly discuss why RL is a better fit (e.g., dynamic environments, long-term planning).\n",
      "\n",
      "---\n",
      "\n",
      "## 4. üß† Technical Contribution and Theoretical Soundness\n",
      "\n",
      "**Key Concerns:** Low technical contribution, trivial extensions of known results, and errors in theoretical claims (e.g., flawed theorems).\n",
      "\n",
      "**Best-Practice Solutions:**\n",
      "\n",
      "1. **Emphasize Novel Contributions**\n",
      "   - Clearly state how the work advances the field (e.g., new algorithms, novel applications).\n",
      "   - Avoid restating known results without justification.\n",
      "\n",
      "2. **Ensure Theoretical Rigor**\n",
      "   - Validate all theorems and proofs with detailed derivations (e.g., equation (3) correction).\n",
      "   - Explicitly state assumptions (e.g., in Big-O notation) and justify their validity.\n",
      "\n",
      "3. **Avoid Trivial Extensions**\n",
      "   - Demonstrate how results extend prior work, rather than rephrasing existing ideas.\n",
      "   - Use literature comparisons to highlight originality.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. üìú Clarity in Equations and Methodology\n",
      "\n",
      "**Key Concerns:** Vague equations (e.g., Equation 2), undefined variables (e.g., z), and insufficient context for complex components (e.g., hierarchical LSTM).\n",
      "\n",
      "**Best-Practice Solutions:**\n",
      "\n",
      "1. **Provide Detailed Explanations**\n",
      "   - Define all variables and terms in equations (e.g., \"z\" = latent representation).\n",
      "   - Use examples or analogies to clarify abstract concepts.\n",
      "\n",
      "2. **Structure Complex Components Gradually**\n",
      "   - Introduce complex elements (e.g., hierarchical LSTM) step-by-step, with visual aids if needed.\n",
      "   - Link each component to its purpose in the overall methodology.\n",
      "\n",
      "3. **Use Consistent Notation**\n",
      "   - Define symbols once and reuse them consistently throughout the paper.\n",
      "   - Avoid ambiguous abbreviations without prior explanation.\n",
      "\n",
      "---\n",
      "\n",
      "## 6. üìâ Robust Theoretical and Experimental Validation\n",
      "\n",
      "**Key Concerns:** Missing ablation studies, inadequate comparisons (e.g., two-time-scale structure), and unclear demonstration of contribution effectiveness.\n",
      "\n",
      "**Best-Practice Solutions:**\n",
      "\n",
      "1. **Conduct Systematic Ablation Studies**\n",
      "   - Isolate the impact of individual components (e.g., progressive training, two-time-scale structure).\n",
      "   - Present results in tables or graphs with clear baselines.\n",
      "\n",
      "2. **Compare with Strong Baselines**\n",
      "   - Include detailed comparisons with state-of-the-art methods.\n",
      "   - Highlight improvements in performance, efficiency, or robustness.\n",
      "\n",
      "3. **Justify Contribution Impact**\n",
      "   - Explain how each contribution addresses specific limitations in prior work.\n",
      "   - Use case studies or scenarios to demonstrate practical benefits.\n",
      "\n",
      "---\n",
      "\n",
      "## 7. üîÑ Implementation and Reproducibility\n",
      "\n",
      "**Key Concerns:** Missing code/hyperparameters, unclear attribution of speed gains, and lack of implementation details.\n",
      "\n",
      "**Best-Practice Solutions:**\n",
      "\n",
      "1. **Share Implementation Details**\n",
      "   - Provide access to code repositories (e.g., GitHub) and document hyperparameters.\n",
      "   - Include configuration files or scripts for reproducibility.\n",
      "\n",
      "2. **Isolate Algorithmic Improvements**\n",
      "   - Distinguish between algorithmic innovations and implementation choices (e.g., speed gains from code optimization vs. design changes).\n",
      "   - Use controlled experiments to validate claims.\n",
      "\n",
      "3. **Document Reproducibility Steps**\n",
      "   - Specify software versions, libraries, and hardware used.\n",
      "   - Provide instructions for replicating experiments in the supplementary materials.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "\n",
    "# Load your previously saved concerns.json\n",
    "with open(\"concerns.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    concern_map = json.load(f)\n",
    "\n",
    "# Load the expected result from the file\n",
    "expected_result = open(\"expected_result.txt\", \"r\", encoding=\"utf-8\")\n",
    "\n",
    "# Flatten all the lists of sentences into one list\n",
    "all_sentences = [sent for sentences in concern_map.values() for sent in sentences]\n",
    "\n",
    "# Removes exact repeats so clusters aren‚Äôt skewed by identical items.\n",
    "unique_sentences = list(dict.fromkeys(all_sentences))\n",
    "\n",
    "print(unique_sentences)\n",
    "\n",
    "def normalize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# Normalize: lowercase, strip punctuation, remove extra whitespace\n",
    "normalized = [normalize(s) for s in unique_sentences]\n",
    "\n",
    "# Remove garbage (like \"okay\", etc) or extremely short items\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove any lines that are too short or made up mostly of stop-words (e.g. ‚Äúand‚Äù, ‚Äúthe‚Äù, ‚Äúof‚Äù)\n",
    "filtered = []\n",
    "for orig, norm in zip(unique_sentences, normalized):\n",
    "    tokens = nltk.word_tokenize(norm)\n",
    "    # keep only if more than 3 non-stop tokens\n",
    "    if sum(1 for t in tokens if t not in stop_words) >= 3:\n",
    "        filtered.append((orig, norm))\n",
    "\n",
    "sentences, norms = zip(*filtered)\n",
    "\n",
    "# EMBEDDINGS\n",
    "# Each sentence is converted into a fixed-length vector that ‚Äúencodes‚Äù its meaning.\n",
    "# Sentences with similar semantics end up closer in this vector space.\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(list(norms), convert_to_tensor=False)\n",
    "\n",
    "# CLUSTERING\n",
    "# Starts with each sentence as its own cluster.\n",
    "# Iteratively merges the two closest clusters until only n_clusters remain.\n",
    "n_clusters = 8\n",
    "clusterer = AgglomerativeClustering(n_clusters=n_clusters) # or KMeans(n_clusters=n_clusters, random_state=0)\n",
    "labels = clusterer.fit_predict(embeddings)\n",
    "\n",
    "# Each cluster (that now is a group of sentences that are similar to each other) has an index.\n",
    "clusters = defaultdict(list)\n",
    "for lab, orig in zip(labels, sentences):\n",
    "    clusters[lab].append(orig)\n",
    "\n",
    "\n",
    "def pick_representative(sent_list):\n",
    "    # simple heuristic: choose the longest (most detailed) sentence\n",
    "    return max(sent_list, key=lambda s: len(s))\n",
    "\n",
    "# For now we will just pick the longest sentence in each cluster as the representative.\n",
    "guidelines = []\n",
    "for lab, items in clusters.items():\n",
    "    rep = pick_representative(items)\n",
    "    guidelines.append(f\"- {rep}\")\n",
    "\n",
    "print(\"\\n\".join(guidelines))\n",
    "final_question = f\"\"\"\n",
    "I have analized a dataset of OpenReview papers with reviews of the papers because I wanted to extract some common concern that the reviewers have, to help authors know in advance what are the common concerns. Now I have a list of general concerns, can you help me creating a nice markdown with the concerns and best solutions? For example, this can be a possible result:\n",
    "<|result|>\n",
    "{expected_result.read()}\n",
    "<|result|>\n",
    "Remember that I want general guidelines, I do not want citations to specific papers or ML-related guidelines. For example These are the concerns:\n",
    "<|concerns|>\n",
    "{\"\\n\".join(guidelines)}\n",
    "<|concerns|>\n",
    "Answer me with only the final guideline markdown.\n",
    "\"\"\"\n",
    "\n",
    "# We can also use more powerful models like ChatGPT\n",
    "response = ollama.chat(model='qwen3:14b', messages = [\n",
    "    {\n",
    "\t\t\"role\": \"user\",\n",
    "\t\t\"content\": final_question\n",
    "\t}\n",
    "])['message']['content']\n",
    "\n",
    "# Strip off any '<‚Äã/think>' section if present\n",
    "if '</think>' in response:\n",
    "\tresponse = response.split('</think>', 1)[1]\n",
    "print(response)\n",
    "\n",
    "with open(\"final_idea_result.md\", \"w\") as f:\n",
    "    f.write(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e75c522",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Limitations & Future Work\n",
    "\n",
    "Our revised pipeline makes guideline generation more scalable, but several challenges remain. Processing large numbers of papers end-to-end can quickly become a bottleneck, and some of our design choices introduce trade-offs:\n",
    "\n",
    "- **Performance & Cost:** Running a 14B-parameter LLM for every paper (plus SBERT for embeddings) is slow and expensive. You‚Äôll need either a beefy GPU cluster or a paid inference API to maintain reasonable throughput.  \n",
    "- **Fixed Number of Clusters:** Using a hard-coded `n_clusters` risks over- or under-segmenting reviewer concerns. In future iterations we should incorporate metrics (e.g. silhouette score) or density-based methods (DBSCAN/HDBSCAN) to choose cluster counts automatically.  \n",
    "- **Simple Representative Heuristic:** We currently pick the longest sentence in each cluster as the ‚Äúheadline‚Äù concern, which may be verbose or overly technical. A more robust approach could select the sentence closest to the cluster centroid or ask a small, focused LLM to synthesize a concise summary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
